{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 2 #number of iterations for each p_obs, put 100 to get the result of the paper (but it takes 4~5 hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from numpy import linalg as LA\n",
    "from sklearn.cluster import KMeans\n",
    "import subprocess\n",
    "import time\n",
    "import math\n",
    "import pandas as pd\n",
    "import pdb\n",
    "\n",
    "class Data_Generator:\n",
    "    \"\"\"Generates synthetic data\"\"\"\n",
    "    def __init__(self, alpha, beta, p_obs, p_list, k, n, m):\n",
    "        ## storing input parameters within the class\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.p_obs = p_obs\n",
    "        self.p_list = p_list\n",
    "        self.k = k\n",
    "        self.n = n\n",
    "        self.m = m\n",
    "        ## define additional class variables\n",
    "        self.U = np.zeros((k, m)) # u_1, u_2, ..., u_K\n",
    "        self.U_set = False\n",
    "        self.n_per_cluster = int(n/k)\n",
    "        self.cluster_id = np.arange(n)//self.n_per_cluster # 000111...(k-1)(k-1)...\n",
    "        \n",
    "        self.M_ground_truth = {}\n",
    "        self.M_train = {}\n",
    "        \n",
    "        self.M_train = None\n",
    "        self.Adj_matrix = None\n",
    "        self.Adj_list = None\n",
    "        \n",
    "        assert( self.n % self.k == 0 ) # Equal division possible\n",
    "        \n",
    "    def set_U(self, U):\n",
    "        self.U = U\n",
    "        self.U_set = True\n",
    "\n",
    "    def generate_rating_data(self):\n",
    "        if self.U_set:\n",
    "            X_full_obs = -1+2*np.array(np.random.random((self.n,self.m)) <= np.repeat(self.U, np.array(np.ones(self.k)*self.n_per_cluster, dtype=int), axis=0), dtype=float)\n",
    "            X_partial_obs = X_full_obs * np.array(np.random.random((self.n,self.m)) <= self.p_obs, dtype=float)\n",
    "            return X_partial_obs\n",
    "        else:\n",
    "            print(\"U is not set yet\")\n",
    "            return None\n",
    "    \n",
    "    def generate_graph(self):\n",
    "        alpha = self.alpha\n",
    "        beta = self.beta\n",
    "        n = self.n\n",
    "        n_per_cluster = self.n_per_cluster\n",
    "        cluster_id = self.cluster_id\n",
    "        Adj_matrix = np.zeros((n,n))   \n",
    "        Adj_list = {}\n",
    "        for i in range(n):\n",
    "            Adj_list[i] = []\n",
    "        \n",
    "        for i in range(n):\n",
    "            for j in range(i+1,n):\n",
    "                if cluster_id[i] == cluster_id[j]:\n",
    "                    if np.random.rand() <= alpha:\n",
    "                        Adj_matrix[i,j] = 1\n",
    "                        Adj_matrix[j,i] = 1\n",
    "                        Adj_list[i].append(j)\n",
    "                        Adj_list[j].append(i)\n",
    "                else:\n",
    "                    if np.random.rand() <= beta:\n",
    "                        Adj_matrix[i,j] = 1\n",
    "                        Adj_matrix[j,i] = 1\n",
    "                        Adj_list[i].append(j)\n",
    "                        Adj_list[j].append(i)\n",
    "        \n",
    "        return Adj_matrix, Adj_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVR:\n",
    "    MAX_N_OF_REFINEMENT_STEPS = 10\n",
    "    \n",
    "    def __init__(self, M_obs, Adj_matrix, Adj_list, n, m, k, p_gt):\n",
    "        self.M_obs = M_obs\n",
    "        self.M_obs_locations = np.where(M_obs != 0)\n",
    "        self.Adj_matrix = Adj_matrix\n",
    "        self.Adj_list = Adj_list\n",
    "        self.n = n\n",
    "        self.m = m\n",
    "        self.k = k #number of clusters of users\n",
    "        self.d = p_gt.size\n",
    "                \n",
    "    def spectral_clustering_and_vote(self, truncation_threshold = 6, local_refinement_flag = False):\n",
    "        M_obs = self.M_obs\n",
    "        M_obs_locations = self.M_obs_locations\n",
    "        Adj = self.Adj_matrix\n",
    "        Adj_original = np.copy(Adj)\n",
    "        \n",
    "        Adj_list = self.Adj_list\n",
    "\n",
    "        n = self.n\n",
    "        m = self.m\n",
    "        k = self.k\n",
    "        d = self.d # number of probabilities p_1,...,p_d\n",
    "        z = 2 # number of possible ratings binary in Alg 1, but will be bigger than 2 in experiment 3\n",
    "        \n",
    "        # Stage 1. Spectral clustering\n",
    "        # Caution: This may be slow for very large n\n",
    "        deg_th = truncation_threshold * np.sum(Adj)/n\n",
    "        heavy_rows = np.where(np.sum(Adj,1) > deg_th)[0]\n",
    "        Adj[heavy_rows,:] = 0\n",
    "        Adj[:,heavy_rows] = 0\n",
    "        dd, vv = sp.linalg.eigs(Adj, k = k)\n",
    "        kmeans = KMeans(n_clusters=k, random_state=0).fit(np.real(vv))\n",
    "        k_mean_results = kmeans.labels_\n",
    "        \n",
    "#        print(\"Stage 1 results\", k_mean_results)\n",
    "        stage1_clustering_results = np.copy(k_mean_results)\n",
    "        \n",
    "        # Stage 2. Majority voting\n",
    "#         k_mean_results = 1-np.array(np.floor(np.arange(0,n)/(n/2)), dtype=int)\n",
    "        \n",
    "        B_est = np.zeros((k, m)) # Caution: The row indices of B_est and B do not match in general\n",
    "        B_ct = np.zeros((k, m, z)) # B_ct(:,:,0) for 0, B_ct(:,:,1) = for 1, and so on, used for finding p_hat\n",
    "        R_ct = np.zeros((k, m, d)) \n",
    "        R_est = np.zeros((k, m)) # estimation of rating matrix from stage 2; u_hat, v_hat\n",
    "        \n",
    "        \n",
    "        for z in range(len(M_obs_locations[0])): # O(pnm)\n",
    "            i = M_obs_locations[0][z] # i\n",
    "            j = M_obs_locations[1][z] # j\n",
    "#             pdb.set_trace()\n",
    "            cluster_idx = k_mean_results[i]\n",
    "            if M_obs[i,j] == -1:\n",
    "                B_ct[cluster_idx, j, 0] += 1\n",
    "            elif M_obs[i,j] == +1:\n",
    "                B_ct[cluster_idx, j, 1] += 1\n",
    "                \n",
    "        for i in range(k):\n",
    "            for j in range(m):\n",
    "                if B_ct[i, j, 1] >= B_ct[i, j, 0]:\n",
    "                    B_est[i,j] = 1\n",
    "                else:\n",
    "                    B_est[i,j] = -1\n",
    "\n",
    "                    \n",
    "        n_ct = 0\n",
    "        diff_ct = 0\n",
    "\n",
    "        for z in range(len(M_obs_locations[0])): # O(pnm)\n",
    "            i = M_obs_locations[0][z] # i\n",
    "            j = M_obs_locations[1][z] # j\n",
    "            cluster_idx = k_mean_results[i]\n",
    "            n_ct += 1\n",
    "            if M_obs[i,j] != B_est[k_mean_results[i],j]:\n",
    "                diff_ct += 1\n",
    "        theta_hat = diff_ct/n_ct\n",
    "#        print(\"theta hat\", theta_hat)\n",
    "        \n",
    "        for i in range(k):\n",
    "            for j in range(m):\n",
    "                if B_est[i,j] == 1:\n",
    "                    R_est[i,j] = 1-theta_hat\n",
    "                else:\n",
    "                    R_est[i,j] = theta_hat\n",
    "                        \n",
    "\n",
    "\n",
    "#        print(\"Stage 2 results\", R_est)\n",
    "\n",
    "        # Stage 3. Local refinement\n",
    "        observed_entries = [None for i in range(n)]\n",
    "        row_sums = Adj_original.sum(axis=1)\n",
    "        \n",
    "#         for i in range(n):\n",
    "#             observed_entries[i] = np.where(~np.isnan(M_train_arr[i,:]))\n",
    "        \n",
    "        stage3_clustering_results = np.copy(k_mean_results)\n",
    "        edges_per_cluster = np.zeros((n, k))\n",
    "        weighted_sum_of_correct_ratings_per_cluster = np.zeros((n, k))\n",
    "        weighted_sum_of_incorrect_ratings_per_cluster = np.zeros((n, k))\n",
    "        number_of_edges_same_cluster = 0\n",
    "        number_of_edges_diff_cluster = 0\n",
    "        \n",
    "        for i in range(n):\n",
    "            for j in range(i+1, n):\n",
    "                if k_mean_results[i] == k_mean_results[j]:\n",
    "                    number_of_edges_same_cluster += Adj_original[i,j]\n",
    "                else:\n",
    "                    number_of_edges_diff_cluster += Adj_original[i,j]\n",
    "                \n",
    "        alpha_hat = 4*number_of_edges_same_cluster/(n)/(n-2)\n",
    "        beta_hat = 4*number_of_edges_diff_cluster/n/n\n",
    "#        print(\"a hat\", alpha_hat)\n",
    "#        print(\"b hat\", beta_hat)\n",
    "\n",
    "    \n",
    "        if local_refinement_flag:\n",
    "            n_of_refinement_steps = 0\n",
    "\n",
    "            while n_of_refinement_steps <= CVR.MAX_N_OF_REFINEMENT_STEPS:\n",
    "                change_flag = False\n",
    "                n_of_refinement_steps += 1\n",
    "#                print(n_of_refinement_steps)\n",
    "                new_k_mean_results = np.copy(stage3_clustering_results)\n",
    "  \n",
    "                nodes_in_each_cluster = {}\n",
    "                for i in range(k):\n",
    "                    nodes_in_each_cluster[i] = np.where(stage3_clustering_results == i)\n",
    "#                 print nodes_in_each_cluster\n",
    "                    \n",
    "                if n_of_refinement_steps == 1: # initial update\n",
    "                    for i in range(n):\n",
    "                        for j in range(i+1, n): # O(n^2)\n",
    "                            if Adj_original[i,j] == 1:\n",
    "                                edges_per_cluster[i, stage3_clustering_results[j]] += 1\n",
    "                                edges_per_cluster[j, stage3_clustering_results[i]] += 1\n",
    "                    list_of_changes = []\n",
    "                    \n",
    "                    \n",
    "                    for z in range(len(M_obs_locations[0])): # O(pnm)\n",
    "                        i = M_obs_locations[0][z] # i\n",
    "                        j = M_obs_locations[1][z] # j\n",
    "                        for l in range(k):\n",
    "                            if M_obs[i,j] == -1:\n",
    "                                weighted_sum_of_incorrect_ratings_per_cluster[i, l] += np.log(1-R_est[l, j])     \n",
    "                            else:\n",
    "                                weighted_sum_of_correct_ratings_per_cluster[i, l] += np.log(R_est[l, j])    \n",
    "                    \n",
    "                else:\n",
    "                    for i in range(n):\n",
    "                        for each_change in list_of_changes: # O(n)\n",
    "                            j, cluster_old, cluster_new = each_change\n",
    "                            if Adj_original[i,j]:\n",
    "                                edges_per_cluster[i, cluster_old] -= 1\n",
    "                                edges_per_cluster[i, cluster_new] += 1\n",
    "#                     pdb.set_trace()\n",
    "                    list_of_changes = []\n",
    "#                 pdb.set_trace()\n",
    "                for i in range(n):\n",
    "#                     print(i)\n",
    "                    likelihood_array = np.zeros(k)\n",
    "                    \n",
    "#                     edges_per_cluster = np.zeros(k)\n",
    "#                     for j in range(n): # O(n^2)\n",
    "#                         if Adj_original[i,j] == 1:\n",
    "#                             edges_per_cluster[stage3_clustering_results[j]] += 1\n",
    "                    for j in range(k): # O(n)\n",
    "                        cluster_idx = j\n",
    "                        deg_internal_1 = edges_per_cluster[i, j]\n",
    "                        deg_internal_0 = (n/k) - deg_internal_1\n",
    "                        deg_external_1 = np.int(row_sums[i]) - deg_internal_1\n",
    "                        deg_external_0 = n-(n/k) - deg_external_1\n",
    "                        \n",
    "                        \n",
    "#                         print(\"Node %d, Cluster %d\" % (i,j))\n",
    "#                         print(deg_internal_1, deg_internal_0, deg_external_1, deg_external_0, weighted_sum_of_correct_ratings, weighted_sum_of_incorrect_ratings)\n",
    "\n",
    "                        likelihood_array[j] = \\\n",
    "                                    np.log((alpha_hat*(1-beta_hat))/(beta_hat*(1-alpha_hat))) * deg_internal_1 + \\\n",
    "                                    weighted_sum_of_correct_ratings_per_cluster[i, j] + \\\n",
    "                                    weighted_sum_of_incorrect_ratings_per_cluster[i, j]\n",
    "#                     pdb.set_trace()\n",
    "                    opt_clustering_assignment = np.argmax(likelihood_array)\n",
    "                    if opt_clustering_assignment != stage3_clustering_results[i]:\n",
    "                        list_of_changes.append((i, stage3_clustering_results[i], opt_clustering_assignment))\n",
    "                        new_k_mean_results[i] = opt_clustering_assignment\n",
    "                        change_flag = True\n",
    "                        \n",
    "#                         pdb.set_trace()\n",
    "#                         print \"Node %d is removed from %d to %d\" % (i, k_mean_results[i], opt_clustering_assignment)\n",
    "\n",
    "                if not change_flag: # nothing happened\n",
    "                    break\n",
    "\n",
    "                stage3_clustering_results = np.copy(new_k_mean_results)\n",
    "                \n",
    "\n",
    "            \n",
    "        return B_est, stage1_clustering_results, stage3_clustering_results, R_est\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## parameters\n",
    "alpha = 0.7\n",
    "beta = 0.3\n",
    "k = 2\n",
    "n = 2000\n",
    "m = 1000\n",
    "p_gt = np.array([0.3, 0.7])\n",
    "d = p_gt.size\n",
    "\n",
    "\n",
    "\n",
    "obs_rate = []\n",
    "max_error = []\n",
    "L1_error = []\n",
    "obs_rate_mean = []\n",
    "max_error_mean = []\n",
    "L1_error_mean = []\n",
    "\n",
    "U = np.array([[0.3]*1000,[0.3]*750+[0.7]*250])\n",
    "\n",
    "\n",
    "## compute the optimal p\n",
    "M_max = 0\n",
    "for i in range(p_gt.size-1):\n",
    "    p_1, p_2 = p_gt[i], p_gt[i+1]\n",
    "    if np.sqrt(p_1 * p_2) + np.sqrt((1-p_1)*(1-p_2)) > M_max:\n",
    "        M_max = np.sqrt(p_1 * p_2) + np.sqrt((1-p_1)*(1-p_2))\n",
    "gamma = np.sum(U[0,:] != U[1,:])/m\n",
    "I_s = -2*np.log(np.sqrt(alpha*beta) + np.sqrt((1-alpha)*(1-beta)))\n",
    "p_opt = max((np.log(n) - n*I_s/2)/(gamma*n), 2*np.log(m)/n)/(1-M_max)\n",
    "p_opt_wo_graph = max(np.log(n)/(gamma*n), 2*np.log(m)/n)/(1-M_max)\n",
    "print((np.log(n))/(gamma*n))\n",
    "print((np.log(n) - n*I_s/2)/(gamma*n))\n",
    "print(2*np.log(m)/n)\n",
    "print(\"I_s\", I_s)\n",
    "print(\"gamma\", gamma)\n",
    "print(\"p_opt\", p_opt)\n",
    "print(\"p_opt_wo_graph\", p_opt_wo_graph)\n",
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "for l in range(5):\n",
    "    p_obs = 0.04+0.02*l\n",
    "    mean_max_err = 0\n",
    "    mean_L1_err = 0\n",
    "    print(l)\n",
    "    now = datetime.now()\n",
    "    current_time = now.strftime(\"%H:%M:%S\")\n",
    "    print(\"Current Time =\", current_time)\n",
    "    \n",
    "    for j in range(T):\n",
    "        dg = Data_Generator(alpha, beta, p_obs, p_gt, k, n, m)\n",
    "        dg.set_U(U)\n",
    "        Adj_matrix, Adj_list = dg.generate_graph()\n",
    "        solver = CVR(dg.generate_rating_data(), Adj_matrix, Adj_list, n, m, k, p_gt)\n",
    "        B_est, stage1_clustering_results, stage3_clustering_results, R_est = solver.spectral_clustering_and_vote(local_refinement_flag = True)\n",
    "        total_err = 0\n",
    "        max_err = 0\n",
    "        ground_truth_cluster = np.array(np.floor(np.arange(0,n)/(n/2)), dtype=int)\n",
    "        for i in range(n):\n",
    "            total_err += np.linalg.norm(R_est[stage3_clustering_results[i]] - U[ground_truth_cluster[i]], ord=1)\n",
    "            if np.max(np.abs(R_est[stage3_clustering_results[i]] - U[ground_truth_cluster[i]])) > max_err:\n",
    "                max_err = np.max(np.abs(R_est[stage3_clustering_results[i]] - U[ground_truth_cluster[i]]))\n",
    "        normalized_L1_norm = total_err/n/m\n",
    "        \n",
    "        obs_rate.append(p_obs)\n",
    "        max_error.append(max_err)\n",
    "        L1_error.append(normalized_L1_norm)\n",
    "        \n",
    "        mean_max_err += max_err\n",
    "        mean_L1_err += normalized_L1_norm\n",
    "        \n",
    "    mean_max_err = mean_max_err/T #it will be 0 after T iterations\n",
    "    mean_L1_err = mean_L1_err/T #it will be 0 after T iterations\n",
    "    \n",
    "    obs_rate_mean.append(p_obs)\n",
    "    max_error_mean.append(mean_max_err)\n",
    "    L1_error_mean.append(mean_L1_err)\n",
    "    \n",
    "print(\"obs_rate\", obs_rate)    \n",
    "print(\"max_error\", max_error) \n",
    "print(\"L1_error\", L1_error)\n",
    "print(\"obs_rate_mean\", obs_rate_mean)\n",
    "print(\"max_error_mean\", max_error_mean)\n",
    "print(\"L1_error_mean\", L1_error_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as io\n",
    "\n",
    "io.savemat('data_4_a_prev',{'obs':obs_rate, 'max':max_error, 'L1':L1_error, 'obs_mean':obs_rate_mean, 'max_mean':max_error_mean, 'L1_mean':L1_error_mean})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "dset = pd.DataFrame({'a':obs_rate_mean, 'b':max_error_mean, 'c':L1_error_mean})\n",
    "\n",
    "plt.scatter(dset['a'], dset['b'], color='red', label = \"(max_err)\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel('Observation Rate')\n",
    "plt.ylabel('Error Rate')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(dset['a'], dset['c'], label = \"(L1_err)\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel('Observation Rate')\n",
    "plt.ylabel('Error Rate')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
