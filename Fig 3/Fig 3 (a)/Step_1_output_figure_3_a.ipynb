{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 2 #number of iterations for each p_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from numpy import linalg as LA\n",
    "from sklearn.cluster import KMeans\n",
    "import subprocess\n",
    "import time\n",
    "import math\n",
    "import pandas as pd\n",
    "import pdb\n",
    "\n",
    "class Data_Generator:\n",
    "    \"\"\"Generates synthetic data\"\"\"\n",
    "    def __init__(self, alpha, beta, p_obs, p_list, k, n, m):\n",
    "        ## storing input parameters within the class\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.p_obs = p_obs\n",
    "        self.p_list = p_list\n",
    "        self.k = k\n",
    "        self.n = n\n",
    "        self.m = m\n",
    "        ## define additional class variables\n",
    "        self.U = np.zeros((k, m)) # u_1, u_2, ..., u_K\n",
    "        self.U_set = False\n",
    "        self.n_per_cluster = int(n/k)\n",
    "        self.cluster_id = np.arange(n)//self.n_per_cluster # 000111...(k-1)(k-1)...\n",
    "        \n",
    "        self.M_ground_truth = {}\n",
    "        self.M_train = {}\n",
    "        \n",
    "        self.M_train = None\n",
    "        self.Adj_list = None\n",
    "        \n",
    "        assert( self.n % self.k == 0 ) # Equal division possible\n",
    "        \n",
    "    def set_U(self, U):\n",
    "        self.U = U\n",
    "        self.U_set = True\n",
    "\n",
    "    def generate_rating_data(self):\n",
    "        if self.U_set:\n",
    "            X_full_obs = -1+2*np.array(np.random.random((self.n,self.m)) <= np.repeat(self.U, np.array(np.ones(self.k)*self.n_per_cluster, dtype=int), axis=0), dtype=float)\n",
    "            X_partial_obs = X_full_obs * np.array(np.random.random((self.n,self.m)) <= self.p_obs, dtype=float)\n",
    "            return X_partial_obs\n",
    "        else:\n",
    "            print(\"U is not set yet\")\n",
    "            return None\n",
    "    \n",
    "    def generate_graph(self):\n",
    "        alpha = self.alpha\n",
    "        beta = self.beta\n",
    "        n = self.n\n",
    "        n_per_cluster = self.n_per_cluster\n",
    "        cluster_id = self.cluster_id\n",
    "        Adj_list = [[] for i in range(3)]\n",
    "\n",
    "                        \n",
    "        for i in range(n):\n",
    "            for j in range(i+1,n):\n",
    "                if cluster_id[i] == cluster_id[j]:\n",
    "                    if np.random.rand() <= alpha:\n",
    "                        Adj_list[0].append(float(1))\n",
    "                        Adj_list[1].append(i)\n",
    "                        Adj_list[2].append(j)\n",
    "                        Adj_list[0].append(float(1))\n",
    "                        Adj_list[1].append(j)\n",
    "                        Adj_list[2].append(i)\n",
    "                else:\n",
    "                    if np.random.rand() <= beta:\n",
    "                        Adj_list[0].append(float(1))\n",
    "                        Adj_list[1].append(i)\n",
    "                        Adj_list[2].append(j)\n",
    "                        Adj_list[0].append(float(1))\n",
    "                        Adj_list[1].append(j)\n",
    "                        Adj_list[2].append(i)                        \n",
    "        \n",
    "        return Adj_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVR:\n",
    "    MAX_N_OF_REFINEMENT_STEPS = 10\n",
    "    \n",
    "    def __init__(self, M_obs, Adj_list, n, m, k, p_gt):\n",
    "        self.M_obs = M_obs\n",
    "        self.M_obs_locations = np.where(M_obs != 0)\n",
    "        self.Adj_list = Adj_list\n",
    "        self.n = n\n",
    "        self.m = m\n",
    "        self.k = k #number of clusters of users\n",
    "        self.d = p_gt.size\n",
    "                \n",
    "    def spectral_clustering_and_vote(self, truncation_threshold = 6, local_refinement_flag = False):\n",
    "        M_obs = self.M_obs\n",
    "        M_obs_locations = self.M_obs_locations\n",
    "        Adj_list = self.Adj_list\n",
    "        \n",
    "        Adj = sp.coo_matrix((Adj_list[0], (Adj_list[1], Adj_list[2])))\n",
    "        Adj_csr = Adj.tocsr()\n",
    "        Adj_original_csr = Adj_csr\n",
    "        Adj_original = np.copy(Adj)\n",
    "        \n",
    "\n",
    "        n = self.n\n",
    "        m = self.m\n",
    "        k = self.k\n",
    "        d = self.d # number of probabilities p_1,...,p_d\n",
    "        z = 2 # number of possible ratings binary in Alg 1, but will be bigger than 2 in experiment 3\n",
    "        \n",
    "        neighbor_list = [[] for x in range(n)]\n",
    "        for i in range(len(Adj_list[0])):\n",
    "            neighbor_list[Adj_list[1][i]].append(Adj_list[2][i])        \n",
    "        \n",
    "        # Stage 1. Spectral clustering\n",
    "        # Caution: This may be slow for very large n\n",
    "        deg_th = truncation_threshold * np.sum(Adj)/n\n",
    "        heavy_rows = np.where(np.sum(Adj,1) > deg_th)[0]\n",
    "        \n",
    "        if len(heavy_rows): \n",
    "            Adj_list_new = [[] for x in range(3)]\n",
    "            for i in range(len(Adj_list[0])):\n",
    "                if Adj_list[1][i] in heavy_rows or Adj_list[2][i] in heavy_rows:\n",
    "                    continue\n",
    "                Adj_list_new[0].append(float(1))\n",
    "                Adj_list_new[1].append(Adj_list[1][i])    # need to change order of i and 1 ?\n",
    "                Adj_list_new[2].append(Adj_list[2][i])\n",
    "        else:\n",
    "            Adj_list_new = Adj_list\n",
    "            \n",
    "        Adj = sp.coo_matrix((Adj_list_new[0], (Adj_list_new[1], Adj_list_new[2])))\n",
    "        dd, vv = sp.linalg.eigs(Adj, k = k)\n",
    "        kmeans = KMeans(n_clusters=k, random_state=0).fit(np.real(vv))\n",
    "        k_mean_results = kmeans.labels_        \n",
    "\n",
    "#        print(\"Stage 1 results\", k_mean_results)\n",
    "        stage1_clustering_results = np.copy(k_mean_results)        \n",
    "        \n",
    "       \n",
    "        \n",
    "        \n",
    "        # Stage 2. Majority voting\n",
    "#         k_mean_results = 1-np.array(np.floor(np.arange(0,n)/(n/2)), dtype=int)\n",
    "        \n",
    "        B_est = np.zeros((k, m)) # Caution: The row indices of B_est and B do not match in general\n",
    "        n_ct = np.zeros((k, m))\n",
    "        B_ct = np.zeros((k, m, z)) # B_ct(:,:,0) for 0, B_ct(:,:,1) = for 1, and so on, used for finding p_hat\n",
    "        R_ct = np.zeros((k, m, d)) \n",
    "        R_est = np.zeros((k, m)) # estimation of rating matrix from stage 2; u_hat, v_hat\n",
    "        \n",
    "        \n",
    "        p_hat = np.zeros(d)\n",
    "            \n",
    "        for i in range(d):\n",
    "            p_hat[i] = p_gt[i]\n",
    "        \n",
    "            \n",
    "#         pdb.set_trace()\n",
    "        \n",
    "        for z in range(len(M_obs_locations[0])): # O(pnm)\n",
    "            i = M_obs_locations[0][z] # i\n",
    "            j = M_obs_locations[1][z] # j\n",
    "            for l in range(d):\n",
    "                cluster_idx = k_mean_results[i]\n",
    "                if M_obs[i,j] == -1:\n",
    "                    R_ct[cluster_idx, j, l] += -np.log(1-p_hat[l])    #use 1.01 instead of 1 to avoid log(0) case\n",
    "                else:\n",
    "                    R_ct[cluster_idx, j, l] += -np.log(p_hat[l])    #use 0.01 instead of 0 to avoid log(0) case\n",
    "        \n",
    "        if np.sum(p_hat < 0) or np.sum(p_hat > 1):\n",
    "            pdb.set_trace()\n",
    "            \n",
    "        emp = []\n",
    "        \n",
    "        for i in range(k):\n",
    "            for j in range(m):\n",
    "                for l in range(d):\n",
    "                    emp.append(R_ct[i, j, l])\n",
    "                R_est[i, j] = p_hat[np.argmin(emp)]\n",
    "                emp = []\n",
    "                                \n",
    "#        print(\"Stage 2 results\", R_est)\n",
    "\n",
    "\n",
    "        # Stage 3. Local refinement\n",
    "        observed_entries = [None for i in range(n)]\n",
    "        row_sums = Adj_csr.sum(axis=1)        \n",
    "#        print(\"row_sums\", row_sums)\n",
    "\n",
    "#         for i in range(n):\n",
    "#             observed_entries[i] = np.where(~np.isnan(M_train_arr[i,:]))\n",
    "        \n",
    "        stage3_clustering_results = np.copy(k_mean_results)\n",
    "        edges_per_cluster = np.zeros((n, k))\n",
    "        weighted_sum_of_correct_ratings_per_cluster = np.zeros((n, k))\n",
    "        weighted_sum_of_incorrect_ratings_per_cluster = np.zeros((n, k))\n",
    "        number_of_edges_same_cluster = 0\n",
    "        number_of_edges_diff_cluster = 0\n",
    "        number_of_total_pairs_same_cluster = 0\n",
    "        number_of_total_pairs_diff_cluster = 0\n",
    "\n",
    "        n_per_cluster_stage1_list = []\n",
    "        for i in range(k):\n",
    "            n_per_cluster_stage1_list.append(len(np.where(k_mean_results==i)[0]))\n",
    "            \n",
    "#        print(\"stage3_n\", n)\n",
    "#        print(\"stage1_n_per_cluster\",n_per_cluster_stage1_list)        \n",
    "        \n",
    "        for i in range(k):\n",
    "            number_of_total_pairs_same_cluster += n_per_cluster_stage1_list[i]*(n_per_cluster_stage1_list[i]-1)/2\n",
    "            \n",
    "        for i in range(k):\n",
    "            for j in range(i+1,k):\n",
    "                number_of_total_pairs_diff_cluster += n_per_cluster_stage1_list[i]*n_per_cluster_stage1_list[j]\n",
    "        \n",
    "                    \n",
    "\n",
    "        for i in range(n):\n",
    "            for each_neighbor in neighbor_list[i]:\n",
    "                if k_mean_results[i] == k_mean_results[each_neighbor]:\n",
    "                    number_of_edges_same_cluster += 1\n",
    "                else:\n",
    "                    number_of_edges_diff_cluster += 1\n",
    "                    \n",
    "        number_of_edges_same_cluster = number_of_edges_same_cluster/2\n",
    "        number_of_edges_diff_cluster = number_of_edges_diff_cluster/2\n",
    "\n",
    "\n",
    "\n",
    "#        print(\"number_of_total_pairs_same_cluster\", number_of_total_pairs_same_cluster)\n",
    "#        print(\"number_of_total_pairs_diff_cluster\", number_of_total_pairs_diff_cluster)\n",
    "#        print(\"number_of_edges_same_cluster\", number_of_edges_same_cluster)\n",
    "#        print(\"number_of_edges_diff_cluster\", number_of_edges_diff_cluster)\n",
    "        alpha_hat = number_of_edges_same_cluster/number_of_total_pairs_same_cluster\n",
    "        beta_hat = number_of_edges_diff_cluster/number_of_total_pairs_diff_cluster\n",
    "\n",
    "#        print(\"a hat\", alpha_hat)\n",
    "#       print(\"b hat\", beta_hat)\n",
    "\n",
    "    \n",
    "        if local_refinement_flag:\n",
    "            n_of_refinement_steps = 0\n",
    "\n",
    "            while n_of_refinement_steps <= CVR.MAX_N_OF_REFINEMENT_STEPS:\n",
    "                change_flag = False\n",
    "                n_of_refinement_steps += 1\n",
    "#                print(n_of_refinement_steps)\n",
    "                new_k_mean_results = np.copy(stage3_clustering_results)\n",
    "  \n",
    "                nodes_in_each_cluster = {}\n",
    "                for i in range(k):\n",
    "                    nodes_in_each_cluster[i] = np.where(stage3_clustering_results == i)\n",
    "#                 print nodes_in_each_cluster\n",
    "                    \n",
    "                if n_of_refinement_steps == 1: # initial update\n",
    "                    for i in range(n):\n",
    "                        for each_neighbor in neighbor_list[i]:\n",
    "                            edges_per_cluster[i, stage3_clustering_results[each_neighbor]] += 1\n",
    "                    list_of_changes = []                    \n",
    "\n",
    "                    \n",
    "                    for z in range(len(M_obs_locations[0])): # O(pnm)\n",
    "                        i = M_obs_locations[0][z] # i\n",
    "                        j = M_obs_locations[1][z] # j\n",
    "                        for l in range(k):\n",
    "                            if M_obs[i,j] == -1:\n",
    "                                weighted_sum_of_incorrect_ratings_per_cluster[i, l] += np.log(1-R_est[l, j])     \n",
    "                            else:\n",
    "                                weighted_sum_of_correct_ratings_per_cluster[i, l] += np.log(R_est[l, j])    \n",
    "                    \n",
    "                else:\n",
    "                    for i in range(n):\n",
    "                        for each_change in list_of_changes: # O(n)\n",
    "                            j, cluster_old, cluster_new = each_change\n",
    "                            if Adj_original_csr[i,j]:\n",
    "                                edges_per_cluster[i, cluster_old] -= 1\n",
    "                                edges_per_cluster[i, cluster_new] += 1\n",
    "#                     pdb.set_trace()\n",
    "                    list_of_changes = []\n",
    "#                 pdb.set_trace()\n",
    "\n",
    "                n_per_cluster_middle_of_stage3_list = []\n",
    "                for i in range(k):\n",
    "                    n_per_cluster_middle_of_stage3_list.append(len(np.where(new_k_mean_results==i)[0]))\n",
    "\n",
    "#               print(\"n_per_cluster_middle_of_stage3\",n_per_cluster_middle_of_stage3_list) \n",
    "\n",
    "                for i in range(n):\n",
    "#                     print(i)\n",
    "                    likelihood_array = np.zeros(k)\n",
    "                    \n",
    "#                     edges_per_cluster = np.zeros(k)\n",
    "#                     for j in range(n): # O(n^2)\n",
    "#                         if Adj_original[i,j] == 1:\n",
    "#                             edges_per_cluster[stage3_clustering_results[j]] += 1\n",
    "\n",
    "\n",
    "\n",
    "                    for j in range(k): # O(n)\n",
    "                        cluster_idx = j\n",
    "                        deg_internal_1 = edges_per_cluster[i, j]\n",
    "                        deg_internal_0 = n_per_cluster_middle_of_stage3_list[j] -1 - deg_internal_1\n",
    "                        deg_external_1 = np.int(row_sums[i]) - deg_internal_1\n",
    "                        deg_external_0 = n-n_per_cluster_middle_of_stage3_list[j] - deg_external_1\n",
    "                        \n",
    "                        \n",
    "#                         print(\"Node %d, Cluster %d\" % (i,j))\n",
    "#                         print(deg_internal_1, deg_internal_0, deg_external_1, deg_external_0, weighted_sum_of_correct_ratings, weighted_sum_of_incorrect_ratings)\n",
    "\n",
    "                        likelihood_array[j] = \\\n",
    "                                    np.log(alpha_hat) * deg_internal_1 + \\\n",
    "                                    np.log(1-alpha_hat) * deg_internal_0 + \\\n",
    "                                    np.log(beta_hat) * deg_external_1 + \\\n",
    "                                    np.log(1-beta_hat) * deg_external_0 + \\\n",
    "                                    weighted_sum_of_correct_ratings_per_cluster[i, j] + \\\n",
    "                                    weighted_sum_of_incorrect_ratings_per_cluster[i, j]\n",
    "#                     pdb.set_trace()\n",
    "                    opt_clustering_assignment = np.argmax(likelihood_array)\n",
    "                    if opt_clustering_assignment != stage3_clustering_results[i]:\n",
    "                        list_of_changes.append((i, stage3_clustering_results[i], opt_clustering_assignment))\n",
    "                        new_k_mean_results[i] = opt_clustering_assignment\n",
    "                        change_flag = True\n",
    "                        \n",
    "#                         pdb.set_trace()\n",
    "#                         print \"Node %d is removed from %d to %d\" % (i, k_mean_results[i], opt_clustering_assignment)\n",
    "\n",
    "                if not change_flag: # nothing happened\n",
    "                    break\n",
    "\n",
    "                stage3_clustering_results = np.copy(new_k_mean_results)                    \n",
    "                \n",
    "\n",
    "        n_per_cluster_stage3_list = []\n",
    "        for i in range(k):\n",
    "            n_per_cluster_stage3_list.append(len(np.where(stage3_clustering_results==i)[0]))    \n",
    "            \n",
    "#        print(\"stage3_n_per_cluster\", n_per_cluster_stage3_list)\n",
    "            \n",
    "        return B_est, p_hat, stage1_clustering_results, stage3_clustering_results, R_est\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10000\n",
    "m = 5000\n",
    "k = 2\n",
    "p_gt = np.array([0.2, 0.5, 0.7])\n",
    "d = p_gt.size\n",
    "\n",
    "M_max = 0\n",
    "for i in range(p_gt.size-1):\n",
    "    p_1, p_2 = p_gt[i], p_gt[i+1]\n",
    "    if np.sqrt(p_1 * p_2) + np.sqrt((1-p_1)*(1-p_2)) > M_max:\n",
    "        M_max = np.sqrt(p_1 * p_2) + np.sqrt((1-p_1)*(1-p_2))\n",
    "gamma = 0.5\n",
    "\n",
    "\n",
    "\n",
    "U = np.array([[0.2]*int(m/4)+[0.5]*int(m/2)+[0.7]*int(m/4),[0.2]*int(m/4)+[0.5]*int(m/4)+[0.7]*int(m/4)+[0.5]*int(m/4)])\n",
    "\n",
    "obs_rate = []\n",
    "I_s_list = []\n",
    "prob_err_list = []\n",
    "prob_suc_list = []\n",
    "p_opt_list = []\n",
    "\n",
    "n_of_t = 10\n",
    "n_of_p = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\n",
    "for i in np.arange(n_of_t):\n",
    "    t = 0.5 + i*(2.0-0.5)/n_of_t # (2-0.5)/50.0\n",
    "    alpha = np.power((np.sqrt(t) + 1), 2)*np.log(n)/n\n",
    "    beta = np.log(n)/n\n",
    "    I_s = -2*np.log(np.sqrt(alpha*beta) + np.sqrt((1-alpha)*(1-beta)))\n",
    "    p_opt = max((np.log(n) - n*I_s/2)/(gamma*m), 2*np.log(m)/n)/(1-M_max)\n",
    "    for j in range(n_of_p):\n",
    "        now = datetime.now()\n",
    "        current_time = now.strftime(\"%H:%M:%S\")\n",
    "        print(j)\n",
    "        print(\"Current Time =\", current_time)\n",
    "        p_obs = 0.05 + j*(0.15-0.05)/n_of_p \n",
    "        prob_ct = 0\n",
    "        for h in range(T):\n",
    "            dg = Data_Generator(alpha, beta, p_obs, p_gt, k, n, m)\n",
    "            dg.set_U(U)\n",
    "            Adj_list = dg.generate_graph()\n",
    "            solver = CVR(dg.generate_rating_data(), Adj_list, n, m, k, p_gt)\n",
    "            B_est, p_hat, stage1_clustering_results, stage3_clustering_results, R_est = solver.spectral_clustering_and_vote(local_refinement_flag = True)\n",
    "            max_err = 0\n",
    "            ground_truth_cluster = np.array(np.floor(np.arange(0,n)/(n/2)), dtype=int)\n",
    "            for i in range(n):\n",
    "                if np.max(np.abs(R_est[stage3_clustering_results[i]] - U[ground_truth_cluster[i]])) > max_err:\n",
    "                    max_err = np.max(np.abs(R_est[stage3_clustering_results[i]] - U[ground_truth_cluster[i]]))\n",
    "            \n",
    "            if max_err != 0:\n",
    "                prob_ct += 1\n",
    "                \n",
    "        prob_err = prob_ct/T\n",
    "        prob_suc = 1 - prob_err\n",
    "        \n",
    "        obs_rate.append(p_obs)\n",
    "        I_s_list.append(I_s)\n",
    "        prob_err_list.append(prob_err)\n",
    "        prob_suc_list.append(prob_suc)\n",
    "        p_opt_list.append(p_opt)\n",
    "        \n",
    "print(\"obs_rate\", obs_rate)\n",
    "print(\"I_s_list\", I_s_list)\n",
    "print(\"prob_err_list\", prob_err_list)\n",
    "print(\"prob_suc_list\", prob_suc_list)\n",
    "print(\"p_opt_list\", p_opt_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as io\n",
    "\n",
    "io.savemat('Figure_2_a_data',{'obs':obs_rate, 'I_s':I_s_list, 'prob_err':prob_err_list, 'prob_suc':prob_suc_list, 'p_opt':p_opt_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from numpy import linalg as LA\n",
    "from sklearn.cluster import KMeans\n",
    "import subprocess\n",
    "import time\n",
    "import math\n",
    "import pandas as pd\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as io\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "loaded = io.loadmat('Figure_2_a_data')\n",
    "\n",
    "I_s_min = 0.0003920299707424321\n",
    "I_s_max = 0.0017784049575601882\n",
    "p_opt_min = 0.13748550444397495\n",
    "p_opt_max = 0.0807558504441226\n",
    "\n",
    "I_s_extended_0 = np.append(np.array([I_s_min]), loaded['I_s'][0])\n",
    "I_s_extended = np.append(I_s_extended_0, np.array([I_s_max]))\n",
    "#print(I_s_extended)\n",
    "#print(type(I_s_extended))\n",
    "\n",
    "p_opt_extended_0 = np.append(np.array([p_opt_min]), loaded['p_opt'][0])\n",
    "p_opt_extended = np.append(p_opt_extended_0, np.array([p_opt_max]))\n",
    "#print(p_opt_extended)\n",
    "#print(type(p_opt_extended))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = pd.DataFrame({'x':loaded['I_s'][0], 'y':loaded['obs'][0], 'z':loaded['prob_err'][0], 'w':loaded['p_opt'][0]})\n",
    "\n",
    "plt.rc('font', family='serif', serif='Times')\n",
    "plt.rc('text', usetex=True)\n",
    "plt.rc('xtick', labelsize=13)\n",
    "plt.rc('ytick', labelsize=13)\n",
    "plt.rc('axes', labelsize=13)\n",
    "plt.rc('axes', linewidth=0.5)\n",
    "mpl.rcParams['patch.linewidth']=0.5\n",
    "\n",
    "\n",
    "#width = 5.98\n",
    "#height = 4.92\n",
    "\n",
    "width = 3.27\n",
    "height = width / 1.29\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.subplots_adjust(left= 0.21, bottom=0.2, right=0.89, top=.93)\n",
    "fig.set_size_inches(width, height)\n",
    "#gray_r  cividis_r\n",
    "sc = plt.scatter(dset['x'], dset['y'], c=dset['z'], cmap='cividis_r', marker = 's', s = 215)\n",
    "cbar = plt.colorbar(sc)\n",
    "cbar.set_label(r'$\\Pr(\\psi_1 (N^{\\Omega}, G)\\neq R)$')\n",
    "plt.clim(0.0,1)\n",
    "plt.plot(I_s_extended, p_opt_extended, linewidth=1.5, color = 'red')\n",
    "plt.xlabel(r'$I_s$')\n",
    "plt.ylabel(r'$p$')\n",
    "plt.xlim(np.min(dset['x'])-(np.max(dset['x'])-np.min(dset['x']))/18, np.max(dset['x'])+(np.max(dset['x'])-np.min(dset['x']))/18)\n",
    "plt.ylim(np.min(dset['y'])-(np.max(dset['y'])-np.min(dset['y']))/18, np.max(dset['y'])+(np.max(dset['y'])-np.min(dset['y']))/18)\n",
    "\n",
    "from matplotlib.ticker import FixedLocator, FixedFormatter\n",
    "\n",
    "y_formatter = FixedFormatter([\"0.06\", \"0.08\", \"0.10\", \"0.12\", \"0.14\"])\n",
    "y_locator = FixedLocator([0.06, 0.08, 0.10, 0.12, 0.14])\n",
    "ax.yaxis.set_major_formatter(y_formatter)\n",
    "ax.yaxis.set_major_locator(y_locator)\n",
    "\n",
    "plt.locator_params(axis='x', nbins=5)\n",
    "\n",
    "\n",
    "plt.savefig('Figure_2_a.eps', format='eps', dpi=1000)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
