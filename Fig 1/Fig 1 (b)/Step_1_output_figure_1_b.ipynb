{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 2 #number of iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generator\n",
    "%matplotlib inline\n",
    "import scipy.io as io\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from numpy import linalg as LA\n",
    "from sklearn.cluster import KMeans\n",
    "import subprocess\n",
    "import time\n",
    "import math\n",
    "import pandas as pd\n",
    "import pdb\n",
    "\n",
    "class Data_Generator:\n",
    "    \"\"\"Generates synthetic data\"\"\"\n",
    "    def __init__(self, p_obs, p_list, k, m):\n",
    "        ## storing input parameters within the class\n",
    "        self.p_obs = p_obs\n",
    "        self.p_list = p_list\n",
    "        self.k = k # number of clusters of users (unequal sizes)\n",
    "        self.m = m\n",
    "        \n",
    "        ## define additional class variables\n",
    "        self.U = np.zeros((k, m)) # u_1, u_2, ..., u_K\n",
    "        self.U_set = False\n",
    "\n",
    "        \n",
    "        self.M_ground_truth = {}\n",
    "        self.M_train = {}\n",
    "        \n",
    "        self.M_train = None\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        z = io.loadmat(\"facebook100/Vassar85.mat\")\n",
    "\n",
    "        idx_list = [[] for i in range(k)]\n",
    "        true_group_index = [0, 2, 3]\n",
    "        for i in range(k):\n",
    "            group_index = true_group_index[i]\n",
    "            idx_list[i] = np.where((z['local_info'][:, -2] == 2006+group_index) & (z['local_info'][:, 0] == 1))[0]\n",
    "        idx_2006_2009_student = np.concatenate(idx_list)  # [24,28,30,...15xx, 10,,,14xx, ... 1499,1506,1513]\n",
    "#        print(\"idx_2006_2009_student\",idx_2006_2009_student)\n",
    "#        print(len(idx_2006_2009_student))\n",
    "\n",
    "\n",
    "        B = z['A']\n",
    "#        print(\"B shape\",B.shape)\n",
    "        C = B[idx_2006_2009_student, :] # will be used for finding the largest connected component only\n",
    "        C = C[:, idx_2006_2009_student]\n",
    "#        print(\"C shape\",C.shape)\n",
    "\n",
    "        n_comp, labels = sp.csgraph.connected_components(C)\n",
    "#        print(\"labels\", labels)\n",
    "#        print(\"0\",np.where(labels==0))\n",
    "#        print(idx_2006_2009_student[np.where(labels==0)])\n",
    "        idx_2006_2009_student_large = idx_2006_2009_student[np.where(labels==0)]  # to ignore inactive users (who do not have friends)\n",
    "#        print(\"the number of connected components\", n_comp)\n",
    "#        print(\"the largest connected component\",idx_2006_2009_student_large)\n",
    "#        print(\"length of the largest connected component\",len(idx_2006_2009_student_large))\n",
    "\n",
    "        B_final = z['A'][idx_2006_2009_student_large, :]\n",
    "        B_final = B_final[:, idx_2006_2009_student_large]\n",
    "        #print(\"B_final[0]\", B_final[0])\n",
    "        #print(\"B_final[1136]\", B_final[1136])\n",
    "        #print(\"B_final.shape\",B_final.shape)\n",
    "        D = B_final.toarray()\n",
    "        #print(\"np.where(D[0] == 1)\", np.where(D[0] == 1))\n",
    "        #print(\"np.where(D[1136] == 1\", np.where(D[1136] == 1))\n",
    "        #print(\"D.shape\",D.shape)\n",
    "        # 4 Clusters: Class 06(225), 07(284), 08(351), 09(277)\n",
    "        cluster_id = np.zeros(len(idx_2006_2009_student_large), dtype='int')\n",
    "\n",
    "        for i in range(k):\n",
    "            group_index = true_group_index[i]\n",
    "            cluster_id[np.where(z['local_info'][idx_2006_2009_student_large, -2] == 2006+group_index)] = i            \n",
    "\n",
    "        n = len(cluster_id)    \n",
    "        self.n = n\n",
    "#        print(\"n\", n)\n",
    "\n",
    "        Adj_matrix = np.zeros((n,n))   \n",
    "        Adj_list = [[] for i in range(3)]\n",
    "\n",
    "        B_final_locations = np.where(D == 1)\n",
    "        #print(\"B_final_loc\", B_final_locations)\n",
    "\n",
    "\n",
    "#        print(\"length\", len(B_final_locations[0]))\n",
    "        for y in range(len(B_final_locations[0])): # O(pnm)\n",
    "            i = B_final_locations[0][y] # i\n",
    "            j = B_final_locations[1][y] # j\n",
    "            if  D[i,j] == 1:\n",
    "                Adj_matrix[i,j] = 1\n",
    "                Adj_list[0].append(float(1))\n",
    "                Adj_list[1].append(i)\n",
    "                Adj_list[2].append(j)\n",
    "\n",
    "        #print(Adj_matrix)\n",
    "        #print(Adj_list)        \n",
    "        self.Adj_matrix = Adj_matrix\n",
    "        self.Adj_list = Adj_list\n",
    "        \n",
    "        \n",
    "        n_per_cluster_list = []\n",
    "        for i in range(k):\n",
    "            n_per_cluster_list.append(len(np.where(cluster_id==i)[0]))\n",
    "        \n",
    "        self.n_per_cluster_list = n_per_cluster_list\n",
    "#        print(\"real_n_per_cluster_list\",self.n_per_cluster_list)\n",
    "        self.cluster_id = cluster_id\n",
    "        \n",
    "    def set_U(self, U):\n",
    "        self.U = U\n",
    "        self.U_set = True\n",
    "\n",
    "    def generate_rating_data(self):\n",
    "        if self.U_set:\n",
    "            X_full_obs = -1+2*np.array(np.random.random((self.n,self.m)) <= np.repeat(self.U, self.n_per_cluster_list, axis=0), dtype=float)\n",
    "            X_partial_obs = X_full_obs * np.array(np.random.random((self.n,self.m)) <= self.p_obs, dtype=float)\n",
    "            return X_partial_obs\n",
    "        else:\n",
    "            print(\"U is not set yet\")\n",
    "            return None\n",
    "    \n",
    "    def generate_graph(self):\n",
    "        Adj_matrix = self.Adj_matrix\n",
    "        Adj_list = self.Adj_list\n",
    "        cluster_id = self.cluster_id\n",
    "        n = self.n\n",
    "        return Adj_matrix, Adj_list, cluster_id, n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#solver\n",
    "class CVR:\n",
    "    MAX_N_OF_REFINEMENT_STEPS = 10\n",
    "    \n",
    "    def __init__(self, M_obs, Adj_matrix, Adj_list, n, m, k, p_gt):\n",
    "        self.M_obs = M_obs\n",
    "        self.M_obs_locations = np.where(M_obs != 0)\n",
    "        self.Adj_matrix = Adj_matrix\n",
    "        self.Adj_list = Adj_list\n",
    "        self.n = n\n",
    "        self.m = m\n",
    "        self.k = k #number of clusters of users\n",
    "        self.d = p_gt.size\n",
    "        \n",
    "                \n",
    "    def spectral_clustering_and_vote(self, truncation_threshold = 6, local_refinement_flag = False):\n",
    "        M_obs = self.M_obs\n",
    "        M_obs_locations = self.M_obs_locations\n",
    "        Adj = self.Adj_matrix\n",
    "        Adj_original = np.copy(Adj)\n",
    "        \n",
    "        Adj_list = self.Adj_list\n",
    "\n",
    "        n = self.n\n",
    "        m = self.m\n",
    "        k = self.k\n",
    "        d = self.d # number of probabilities p_1,...,p_d\n",
    "        z = 2 # number of possible ratings binary in Alg 1, but will be bigger than 2 in experiment 3\n",
    "        \n",
    "        # Stage 1. Spectral clustering\n",
    "        # Caution: This may be slow for very large n\n",
    "        deg_th = truncation_threshold * np.sum(Adj)/n\n",
    "        heavy_rows = np.where(np.sum(Adj,1) > deg_th)[0]\n",
    "        Adj[heavy_rows,:] = 0\n",
    "        Adj[:,heavy_rows] = 0\n",
    "        dd, vv = sp.linalg.eigs(Adj, k = k)\n",
    "        kmeans = KMeans(n_clusters=k, random_state=0).fit(np.real(vv))\n",
    "        k_mean_results = kmeans.labels_\n",
    "        \n",
    "#        print(\"Stage 1 results\", k_mean_results)\n",
    "        stage1_clustering_results = np.copy(k_mean_results)\n",
    "        \n",
    "        # Stage 2. Majority voting\n",
    "#         k_mean_results = 1-np.array(np.floor(np.arange(0,n)/(n/2)), dtype=int)\n",
    "        \n",
    "        B_est = np.zeros((k, m)) # Caution: The row indices of B_est and B do not match in general\n",
    "        n_ct = np.zeros((k, m))\n",
    "        B_ct = np.zeros((k, m, z)) # B_ct(:,:,0) for 0, B_ct(:,:,1) = for 1, and so on, used for finding p_hat\n",
    "        R_ct = np.zeros((k, m, d)) \n",
    "        R_est = np.zeros((k, m)) # estimation of rating matrix from stage 2; u_hat, v_hat\n",
    "        \n",
    "        \n",
    "        for z in range(len(M_obs_locations[0])): # O(pnm)\n",
    "            i = M_obs_locations[0][z] # i\n",
    "            j = M_obs_locations[1][z] # j\n",
    "#             pdb.set_trace()\n",
    "            cluster_idx = k_mean_results[i]\n",
    "            n_ct[cluster_idx, j] += 1\n",
    "            if M_obs[i,j] == -1:\n",
    "                B_ct[cluster_idx, j, 0] += 1\n",
    "            elif M_obs[i,j] == +1:\n",
    "                B_ct[cluster_idx, j, 1] += 1\n",
    "\n",
    "                \n",
    "        for i in range(k):\n",
    "            for j in range(m):\n",
    "                if n_ct[i, j] == 0:\n",
    "                    B_est[i,j] = -1\n",
    "                else:\n",
    "                    B_est[i,j] = (B_ct[i,j,1])/n_ct[i, j]\n",
    "                    \n",
    "#        for i in range(k):\n",
    "#            for j in range(m):\n",
    "#                B_est[i,j] = (B_ct[i,j,1])/n_ct[i, j]\n",
    "        \n",
    "\n",
    "        a = []   # a_j, a'_j in Alg 1\n",
    "        b = []\n",
    "        c_r = [] # r_1,...,r_d in Alg 1\n",
    "        c_l = [] # l_1,...,l_d in Alg 1\n",
    "        p_hat = np.zeros(d)\n",
    "#        m_0 = m\n",
    "#        ran = np.random.choice(m, m_0, replace=False)\n",
    "\n",
    "        m_0 = 5*d*int(np.ceil(np.log(m)))\n",
    "        ran = np.random.choice(m, m_0)\n",
    "\n",
    "        for i in range(k):\n",
    "            for j in range(m_0):\n",
    "                if B_est[i,ran[j]] >= 0:\n",
    "                    a.append(B_est[i,ran[j]])\n",
    "                    \n",
    "#        for i in range(k):\n",
    "#            for j in range(m_0):\n",
    "#                a.append(B_est[i,ran[j]])\n",
    "\n",
    "        a.sort()\n",
    "        \n",
    "        for i in range(len(a)-1):\n",
    "            b.append(a[i+1]-a[i])\n",
    "\n",
    "#         pdb.set_trace()\n",
    "        for i in range(d-1):\n",
    "            b[np.argmax(b)] = -1\n",
    "        \n",
    "        for i in range(len(a)-1):\n",
    "            if b[i] == -1:\n",
    "                c_r.append(i)\n",
    "                \n",
    "        c_r.append(len(a)-1)\n",
    "        c_l.append(0)\n",
    "        \n",
    "        for i in range(d-1):\n",
    "            c_l.append(c_r[i]+1)\n",
    "            \n",
    "        for i in range(d):\n",
    "            for j in np.arange(c_l[i],c_r[i]+1):\n",
    "                p_hat[i] += a[j]\n",
    "            p_hat[i] = p_hat[i]/(c_r[i]+1-c_l[i])\n",
    "            if p_hat[i] == 0:\n",
    "                p_hat[i] = 0.00001\n",
    "            if p_hat[i] == 1:\n",
    "                p_hat[i] = 0.99999\n",
    "#        print(\"p_hat\", p_hat)\n",
    "                \n",
    "\n",
    "#         pdb.set_trace()\n",
    "        \n",
    "        for z in range(len(M_obs_locations[0])): # O(pnm)\n",
    "            i = M_obs_locations[0][z] # i\n",
    "            j = M_obs_locations[1][z] # j\n",
    "            for l in range(d):\n",
    "                cluster_idx = k_mean_results[i]\n",
    "                if M_obs[i,j] == -1:\n",
    "                    R_ct[cluster_idx, j, l] += -np.log(1-p_hat[l])    #use 1.01 instead of 1 to avoid log(0) case\n",
    "                else:\n",
    "                    R_ct[cluster_idx, j, l] += -np.log(p_hat[l])    #use 0.01 instead of 0 to avoid log(0) case\n",
    "        \n",
    "        if np.sum(p_hat < 0) or np.sum(p_hat > 1):\n",
    "            pdb.set_trace()\n",
    "        emp = []\n",
    "        \n",
    "        for i in range(k):\n",
    "            for j in range(m):\n",
    "                for l in range(d):\n",
    "                    emp.append(R_ct[i, j, l])\n",
    "                R_est[i, j] = p_hat[np.argmin(emp)]\n",
    "                emp = []\n",
    "#        print(\"Stage 2 results\", R_est)\n",
    "\n",
    "        # Stage 3. Local refinement\n",
    "        observed_entries = [None for i in range(n)]\n",
    "        row_sums = Adj_original.sum(axis=1)\n",
    "#        print(\"row_sums\", row_sums)\n",
    "\n",
    "#         for i in range(n):\n",
    "#             observed_entries[i] = np.where(~np.isnan(M_train_arr[i,:]))\n",
    "        \n",
    "        stage3_clustering_results = np.copy(k_mean_results)\n",
    "        edges_per_cluster = np.zeros((n, k))\n",
    "        weighted_sum_of_correct_ratings_per_cluster = np.zeros((n, k))\n",
    "        weighted_sum_of_incorrect_ratings_per_cluster = np.zeros((n, k))\n",
    "        number_of_edges_same_cluster = 0\n",
    "        number_of_edges_diff_cluster = 0\n",
    "        number_of_total_pairs_same_cluster = 0\n",
    "        number_of_total_pairs_diff_cluster = 0\n",
    "\n",
    "        n_per_cluster_stage1_list = []\n",
    "        for i in range(k):\n",
    "            n_per_cluster_stage1_list.append(len(np.where(k_mean_results==i)[0]))\n",
    "            \n",
    "#        print(\"stage3_n\", n)\n",
    "#        print(\"stage1_n_per_cluster\",n_per_cluster_stage1_list)        \n",
    "        \n",
    "        for i in range(k):\n",
    "            number_of_total_pairs_same_cluster += n_per_cluster_stage1_list[i]*(n_per_cluster_stage1_list[i]-1)/2\n",
    "            \n",
    "        for i in range(k):\n",
    "            for j in range(i+1,k):\n",
    "                number_of_total_pairs_diff_cluster += n_per_cluster_stage1_list[i]*n_per_cluster_stage1_list[j]\n",
    "        \n",
    "        for i in range(n):\n",
    "            for j in range(i+1, n):\n",
    "                if k_mean_results[i] == k_mean_results[j]:\n",
    "                    number_of_edges_same_cluster += Adj_original[i,j]\n",
    "                else:\n",
    "                    number_of_edges_diff_cluster += Adj_original[i,j]\n",
    "                \n",
    "#        print(\"number_of_total_pairs_same_cluster\", number_of_total_pairs_same_cluster)\n",
    "#        print(\"number_of_total_pairs_diff_cluster\", number_of_total_pairs_diff_cluster)\n",
    "#        print(\"number_of_edges_same_cluster\", number_of_edges_same_cluster)\n",
    "#        print(\"number_of_edges_diff_cluster\", number_of_edges_diff_cluster)\n",
    "        alpha_hat = number_of_edges_same_cluster/number_of_total_pairs_same_cluster\n",
    "        beta_hat = number_of_edges_diff_cluster/number_of_total_pairs_diff_cluster\n",
    "#        print(\"a hat\", alpha_hat)\n",
    "#        print(\"b hat\", beta_hat)\n",
    "\n",
    "    \n",
    "        if local_refinement_flag:\n",
    "            n_of_refinement_steps = 0\n",
    "\n",
    "            while n_of_refinement_steps <= CVR.MAX_N_OF_REFINEMENT_STEPS:\n",
    "                change_flag = False\n",
    "                n_of_refinement_steps += 1\n",
    "#                print(n_of_refinement_steps)\n",
    "                new_k_mean_results = np.copy(stage3_clustering_results)\n",
    "  \n",
    "                nodes_in_each_cluster = {}\n",
    "                for i in range(k):\n",
    "                    nodes_in_each_cluster[i] = np.where(stage3_clustering_results == i)\n",
    "#                 print nodes_in_each_cluster\n",
    "                    \n",
    "                if n_of_refinement_steps == 1: # initial update\n",
    "                    for i in range(n):\n",
    "                        for j in range(i+1, n): # O(n^2)\n",
    "                            if Adj_original[i,j] == 1:\n",
    "                                edges_per_cluster[i, stage3_clustering_results[j]] += 1\n",
    "                                edges_per_cluster[j, stage3_clustering_results[i]] += 1\n",
    "                    list_of_changes = []\n",
    "                    \n",
    "                    \n",
    "                    for z in range(len(M_obs_locations[0])): # O(pnm)\n",
    "                        i = M_obs_locations[0][z] # i\n",
    "                        j = M_obs_locations[1][z] # j\n",
    "                        for l in range(k):\n",
    "                            if M_obs[i,j] == -1:\n",
    "                                weighted_sum_of_incorrect_ratings_per_cluster[i, l] += np.log(1-R_est[l, j])     \n",
    "                            else:\n",
    "                                weighted_sum_of_correct_ratings_per_cluster[i, l] += np.log(R_est[l, j])    \n",
    "                    \n",
    "                else:\n",
    "                    for i in range(n):\n",
    "                        for each_change in list_of_changes: # O(n)\n",
    "                            j, cluster_old, cluster_new = each_change\n",
    "                            if Adj_original[i,j]:\n",
    "                                edges_per_cluster[i, cluster_old] -= 1\n",
    "                                edges_per_cluster[i, cluster_new] += 1\n",
    "#                     pdb.set_trace()\n",
    "                    list_of_changes = []\n",
    "#                 pdb.set_trace()\n",
    "\n",
    "                n_per_cluster_middle_of_stage3_list = []\n",
    "                for i in range(k):\n",
    "                    n_per_cluster_middle_of_stage3_list.append(len(np.where(new_k_mean_results==i)[0]))\n",
    "\n",
    "#                print(\"n_per_cluster_middle_of_stage3\",n_per_cluster_middle_of_stage3_list) \n",
    "\n",
    "                for i in range(n):\n",
    "#                     print(i)\n",
    "                    likelihood_array = np.zeros(k)\n",
    "                    \n",
    "#                     edges_per_cluster = np.zeros(k)\n",
    "#                     for j in range(n): # O(n^2)\n",
    "#                         if Adj_original[i,j] == 1:\n",
    "#                             edges_per_cluster[stage3_clustering_results[j]] += 1\n",
    "\n",
    "\n",
    "\n",
    "                    for j in range(k): # O(n)\n",
    "                        cluster_idx = j\n",
    "                        deg_internal_1 = edges_per_cluster[i, j]\n",
    "                        deg_internal_0 = n_per_cluster_middle_of_stage3_list[j] -1 - deg_internal_1\n",
    "                        deg_external_1 = np.int(row_sums[i]) - deg_internal_1\n",
    "                        deg_external_0 = n-n_per_cluster_middle_of_stage3_list[j] - deg_external_1\n",
    "                        \n",
    "                        \n",
    "#                         print(\"Node %d, Cluster %d\" % (i,j))\n",
    "#                         print(deg_internal_1, deg_internal_0, deg_external_1, deg_external_0, weighted_sum_of_correct_ratings, weighted_sum_of_incorrect_ratings)\n",
    "\n",
    "                        likelihood_array[j] = \\\n",
    "                                    np.log(alpha_hat) * deg_internal_1 + \\\n",
    "                                    np.log(1-alpha_hat) * deg_internal_0 + \\\n",
    "                                    np.log(beta_hat) * deg_external_1 + \\\n",
    "                                    np.log(1-beta_hat) * deg_external_0 + \\\n",
    "                                    weighted_sum_of_correct_ratings_per_cluster[i, j] + \\\n",
    "                                    weighted_sum_of_incorrect_ratings_per_cluster[i, j]\n",
    "#                     pdb.set_trace()\n",
    "                    opt_clustering_assignment = np.argmax(likelihood_array)\n",
    "                    if opt_clustering_assignment != stage3_clustering_results[i]:\n",
    "                        list_of_changes.append((i, stage3_clustering_results[i], opt_clustering_assignment))\n",
    "                        new_k_mean_results[i] = opt_clustering_assignment\n",
    "                        change_flag = True\n",
    "                        \n",
    "#                         pdb.set_trace()\n",
    "#                         print \"Node %d is removed from %d to %d\" % (i, k_mean_results[i], opt_clustering_assignment)\n",
    "\n",
    "                if not change_flag: # nothing happened\n",
    "                    break\n",
    "\n",
    "                stage3_clustering_results = np.copy(new_k_mean_results)                    \n",
    "                \n",
    "\n",
    "        n_per_cluster_stage3_list = []\n",
    "        for i in range(k):\n",
    "            n_per_cluster_stage3_list.append(len(np.where(stage3_clustering_results==i)[0]))    \n",
    "            \n",
    "#        print(\"stage3_n_per_cluster\", n_per_cluster_stage3_list)\n",
    "\n",
    "\n",
    "        # Stage 4. iteration of p_hat estimation\n",
    "\n",
    "        \n",
    "        B_est = np.zeros((k, m)) # Caution: The row indices of B_est and B do not match in general\n",
    "        n_ct = np.zeros((k, m))\n",
    "        B_ct = np.zeros((k, m, z)) # B_ct(:,:,0) for 0, B_ct(:,:,1) = for 1, and so on, used for finding p_hat\n",
    "        R_ct = np.zeros((k, m, d)) \n",
    "        R_est = np.zeros((k, m)) # estimation of rating matrix from stage 2; u_hat, v_hat\n",
    "        \n",
    "        \n",
    "        for z in range(len(M_obs_locations[0])): # O(pnm)\n",
    "            i = M_obs_locations[0][z] # i\n",
    "            j = M_obs_locations[1][z] # j\n",
    "#             pdb.set_trace()\n",
    "            cluster_idx = stage3_clustering_results[i]\n",
    "            n_ct[cluster_idx, j] += 1\n",
    "            if M_obs[i,j] == -1:\n",
    "                B_ct[cluster_idx, j, 0] += 1\n",
    "            elif M_obs[i,j] == +1:\n",
    "                B_ct[cluster_idx, j, 1] += 1\n",
    "\n",
    "                \n",
    "        for i in range(k):\n",
    "            for j in range(m):\n",
    "                if n_ct[i, j] == 0:\n",
    "                    B_est[i,j] = -1\n",
    "                else:\n",
    "                    B_est[i,j] = (B_ct[i,j,1])/n_ct[i, j]\n",
    "                    \n",
    "#        for i in range(k):\n",
    "#            for j in range(m):\n",
    "#                B_est[i,j] = (B_ct[i,j,1])/n_ct[i, j]\n",
    "        \n",
    "\n",
    "        a = []   # a_j, a'_j in Alg 1\n",
    "        b = []\n",
    "        c_r = [] # r_1,...,r_d in Alg 1\n",
    "        c_l = [] # l_1,...,l_d in Alg 1\n",
    "        p_hat = np.zeros(d)\n",
    "#        m_0 = m\n",
    "#        ran = np.random.choice(m, m_0, replace=False)\n",
    "\n",
    "        m_0 = 5*d*int(np.ceil(np.log(m)))\n",
    "        ran = np.random.choice(m, m_0)\n",
    "\n",
    "        for i in range(k):\n",
    "            for j in range(m_0):\n",
    "                if B_est[i,ran[j]] >= 0:\n",
    "                    a.append(B_est[i,ran[j]])\n",
    "                    \n",
    "#        for i in range(k):\n",
    "#            for j in range(m_0):\n",
    "#                a.append(B_est[i,ran[j]])\n",
    "\n",
    "        a.sort()\n",
    "        \n",
    "        for i in range(len(a)-1):\n",
    "            b.append(a[i+1]-a[i])\n",
    "\n",
    "#         pdb.set_trace()\n",
    "        for i in range(d-1):\n",
    "            b[np.argmax(b)] = -1\n",
    "        \n",
    "        for i in range(len(a)-1):\n",
    "            if b[i] == -1:\n",
    "                c_r.append(i)\n",
    "                \n",
    "        c_r.append(len(a)-1)\n",
    "        c_l.append(0)\n",
    "        \n",
    "        for i in range(d-1):\n",
    "            c_l.append(c_r[i]+1)\n",
    "            \n",
    "        for i in range(d):\n",
    "            for j in np.arange(c_l[i],c_r[i]+1):\n",
    "                p_hat[i] += a[j]\n",
    "            p_hat[i] = p_hat[i]/(c_r[i]+1-c_l[i])\n",
    "            if p_hat[i] == 0:\n",
    "                p_hat[i] = 0.00001\n",
    "            if p_hat[i] == 1:\n",
    "                p_hat[i] = 0.99999\n",
    "#        print(\"p_hat\", p_hat)\n",
    "                \n",
    "# k_mean_results\n",
    "#         pdb.set_trace()\n",
    "        \n",
    "        for z in range(len(M_obs_locations[0])): # O(pnm)\n",
    "            i = M_obs_locations[0][z] # i\n",
    "            j = M_obs_locations[1][z] # j\n",
    "            for l in range(d):\n",
    "                cluster_idx = stage3_clustering_results[i]\n",
    "                if M_obs[i,j] == -1:\n",
    "                    R_ct[cluster_idx, j, l] += -np.log(1-p_hat[l])    #use 1.01 instead of 1 to avoid log(0) case\n",
    "                else:\n",
    "                    R_ct[cluster_idx, j, l] += -np.log(p_hat[l])    #use 0.01 instead of 0 to avoid log(0) case\n",
    "        \n",
    "        if np.sum(p_hat < 0) or np.sum(p_hat > 1):\n",
    "            pdb.set_trace()\n",
    "        emp = []\n",
    "        \n",
    "        for i in range(k):\n",
    "            for j in range(m):\n",
    "                for l in range(d):\n",
    "                    emp.append(R_ct[i, j, l])\n",
    "                R_est[i, j] = p_hat[np.argmin(emp)]\n",
    "                emp = []\n",
    "\n",
    "        # Stage 5. Iteration of local refinement \n",
    "        observed_entries = [None for i in range(n)]\n",
    "        row_sums = Adj_original.sum(axis=1)\n",
    "#        print(\"row_sums\", row_sums)\n",
    "\n",
    "#         for i in range(n):\n",
    "#             observed_entries[i] = np.where(~np.isnan(M_train_arr[i,:]))\n",
    "        \n",
    "#        stage3_clustering_results = np.copy(k_mean_results)\n",
    "        edges_per_cluster = np.zeros((n, k))\n",
    "        weighted_sum_of_correct_ratings_per_cluster = np.zeros((n, k))\n",
    "        weighted_sum_of_incorrect_ratings_per_cluster = np.zeros((n, k))\n",
    "        number_of_edges_same_cluster = 0\n",
    "        number_of_edges_diff_cluster = 0\n",
    "        number_of_total_pairs_same_cluster = 0\n",
    "        number_of_total_pairs_diff_cluster = 0\n",
    "\n",
    "        n_per_cluster_stage1_list = []\n",
    "        for i in range(k):\n",
    "            n_per_cluster_stage1_list.append(len(np.where(stage3_clustering_results==i)[0]))\n",
    "            \n",
    "#        print(\"stage3_n\", n)\n",
    "#        print(\"stage1_n_per_cluster\",n_per_cluster_stage1_list)        \n",
    "        \n",
    "        for i in range(k):\n",
    "            number_of_total_pairs_same_cluster += n_per_cluster_stage1_list[i]*(n_per_cluster_stage1_list[i]-1)/2\n",
    "            \n",
    "        for i in range(k):\n",
    "            for j in range(i+1,k):\n",
    "                number_of_total_pairs_diff_cluster += n_per_cluster_stage1_list[i]*n_per_cluster_stage1_list[j]\n",
    "        \n",
    "        for i in range(n):\n",
    "            for j in range(i+1, n):\n",
    "                if stage3_clustering_results[i] == stage3_clustering_results[j]:\n",
    "                    number_of_edges_same_cluster += Adj_original[i,j]\n",
    "                else:\n",
    "                    number_of_edges_diff_cluster += Adj_original[i,j]\n",
    "                \n",
    "#        print(\"number_of_total_pairs_same_cluster\", number_of_total_pairs_same_cluster)\n",
    "#        print(\"number_of_total_pairs_diff_cluster\", number_of_total_pairs_diff_cluster)\n",
    "#        print(\"number_of_edges_same_cluster\", number_of_edges_same_cluster)\n",
    "#        print(\"number_of_edges_diff_cluster\", number_of_edges_diff_cluster)\n",
    "        alpha_hat = number_of_edges_same_cluster/number_of_total_pairs_same_cluster\n",
    "        beta_hat = number_of_edges_diff_cluster/number_of_total_pairs_diff_cluster\n",
    "#        print(\"a hat\", alpha_hat)\n",
    "#        print(\"b hat\", beta_hat)\n",
    "\n",
    "    \n",
    "        if local_refinement_flag:\n",
    "            n_of_refinement_steps = 0\n",
    "\n",
    "            while n_of_refinement_steps <= CVR.MAX_N_OF_REFINEMENT_STEPS:\n",
    "                change_flag = False\n",
    "                n_of_refinement_steps += 1\n",
    "#                print(n_of_refinement_steps)\n",
    "                new_k_mean_results = np.copy(stage3_clustering_results)\n",
    "  \n",
    "                nodes_in_each_cluster = {}\n",
    "                for i in range(k):\n",
    "                    nodes_in_each_cluster[i] = np.where(stage3_clustering_results == i)\n",
    "#                 print nodes_in_each_cluster\n",
    "                    \n",
    "                if n_of_refinement_steps == 1: # initial update\n",
    "                    for i in range(n):\n",
    "                        for j in range(i+1, n): # O(n^2)\n",
    "                            if Adj_original[i,j] == 1:\n",
    "                                edges_per_cluster[i, stage3_clustering_results[j]] += 1\n",
    "                                edges_per_cluster[j, stage3_clustering_results[i]] += 1\n",
    "                    list_of_changes = []\n",
    "                    \n",
    "                    \n",
    "                    for z in range(len(M_obs_locations[0])): # O(pnm)\n",
    "                        i = M_obs_locations[0][z] # i\n",
    "                        j = M_obs_locations[1][z] # j\n",
    "                        for l in range(k):\n",
    "                            if M_obs[i,j] == -1:\n",
    "                                weighted_sum_of_incorrect_ratings_per_cluster[i, l] += np.log(1-R_est[l, j])     \n",
    "                            else:\n",
    "                                weighted_sum_of_correct_ratings_per_cluster[i, l] += np.log(R_est[l, j])    \n",
    "                    \n",
    "                else:\n",
    "                    for i in range(n):\n",
    "                        for each_change in list_of_changes: # O(n)\n",
    "                            j, cluster_old, cluster_new = each_change\n",
    "                            if Adj_original[i,j]:\n",
    "                                edges_per_cluster[i, cluster_old] -= 1\n",
    "                                edges_per_cluster[i, cluster_new] += 1\n",
    "#                     pdb.set_trace()\n",
    "                    list_of_changes = []\n",
    "#                 pdb.set_trace()\n",
    "\n",
    "                n_per_cluster_middle_of_stage3_list = []\n",
    "                for i in range(k):\n",
    "                    n_per_cluster_middle_of_stage3_list.append(len(np.where(new_k_mean_results==i)[0]))\n",
    "\n",
    "#                print(\"n_per_cluster_middle_of_stage3\",n_per_cluster_middle_of_stage3_list) \n",
    "\n",
    "                for i in range(n):\n",
    "#                     print(i)\n",
    "                    likelihood_array = np.zeros(k)\n",
    "                    \n",
    "#                     edges_per_cluster = np.zeros(k)\n",
    "#                     for j in range(n): # O(n^2)\n",
    "#                         if Adj_original[i,j] == 1:\n",
    "#                             edges_per_cluster[stage3_clustering_results[j]] += 1\n",
    "\n",
    "\n",
    "\n",
    "                    for j in range(k): # O(n)\n",
    "                        cluster_idx = j\n",
    "                        deg_internal_1 = edges_per_cluster[i, j]\n",
    "                        deg_internal_0 = n_per_cluster_middle_of_stage3_list[j] -1 - deg_internal_1\n",
    "                        deg_external_1 = np.int(row_sums[i]) - deg_internal_1\n",
    "                        deg_external_0 = n-n_per_cluster_middle_of_stage3_list[j] - deg_external_1\n",
    "                        \n",
    "                        \n",
    "#                         print(\"Node %d, Cluster %d\" % (i,j))\n",
    "#                         print(deg_internal_1, deg_internal_0, deg_external_1, deg_external_0, weighted_sum_of_correct_ratings, weighted_sum_of_incorrect_ratings)\n",
    "\n",
    "                        likelihood_array[j] = \\\n",
    "                                    np.log(alpha_hat) * deg_internal_1 + \\\n",
    "                                    np.log(1-alpha_hat) * deg_internal_0 + \\\n",
    "                                    np.log(beta_hat) * deg_external_1 + \\\n",
    "                                    np.log(1-beta_hat) * deg_external_0 + \\\n",
    "                                    weighted_sum_of_correct_ratings_per_cluster[i, j] + \\\n",
    "                                    weighted_sum_of_incorrect_ratings_per_cluster[i, j]\n",
    "#                     pdb.set_trace()\n",
    "                    opt_clustering_assignment = np.argmax(likelihood_array)\n",
    "                    if opt_clustering_assignment != stage3_clustering_results[i]:\n",
    "                        list_of_changes.append((i, stage3_clustering_results[i], opt_clustering_assignment))\n",
    "                        new_k_mean_results[i] = opt_clustering_assignment\n",
    "                        change_flag = True\n",
    "                        \n",
    "#                         pdb.set_trace()\n",
    "#                         print \"Node %d is removed from %d to %d\" % (i, k_mean_results[i], opt_clustering_assignment)\n",
    "\n",
    "                if not change_flag: # nothing happened\n",
    "                    break\n",
    "\n",
    "                stage3_clustering_results = np.copy(new_k_mean_results)                    \n",
    "                \n",
    "\n",
    "        n_per_cluster_stage3_list = []\n",
    "        for i in range(k):\n",
    "            n_per_cluster_stage3_list.append(len(np.where(stage3_clustering_results==i)[0]))                \n",
    "                \n",
    "\n",
    "\n",
    "                \n",
    "        return B_est, p_hat, stage1_clustering_results, stage3_clustering_results, R_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#solver\n",
    "class CVR_Ahns:\n",
    "    MAX_N_OF_REFINEMENT_STEPS = 10\n",
    "    \n",
    "    def __init__(self, M_obs, Adj_matrix, Adj_list, n, m, k, p_gt):\n",
    "        self.M_obs = M_obs\n",
    "        self.M_obs_locations = np.where(M_obs != 0)\n",
    "        self.Adj_matrix = Adj_matrix\n",
    "        self.Adj_list = Adj_list\n",
    "        self.n = n\n",
    "        self.m = m\n",
    "        self.k = k #number of clusters of users\n",
    "        self.d = p_gt.size\n",
    "        \n",
    "                \n",
    "    def spectral_clustering_and_vote(self, truncation_threshold = 6, local_refinement_flag = False):\n",
    "        M_obs = self.M_obs\n",
    "        M_obs_locations = self.M_obs_locations\n",
    "        Adj = self.Adj_matrix\n",
    "        Adj_original = np.copy(Adj)\n",
    "        \n",
    "        Adj_list = self.Adj_list\n",
    "\n",
    "        n = self.n\n",
    "        m = self.m\n",
    "        k = self.k\n",
    "        d = self.d # number of probabilities p_1,...,p_d\n",
    "        z = 2 # number of possible ratings binary in Alg 1, but will be bigger than 2 in experiment 3\n",
    "        \n",
    "        # Stage 1. Spectral clustering\n",
    "        # Caution: This may be slow for very large n\n",
    "        deg_th = truncation_threshold * np.sum(Adj)/n\n",
    "        heavy_rows = np.where(np.sum(Adj,1) > deg_th)[0]\n",
    "        Adj[heavy_rows,:] = 0\n",
    "        Adj[:,heavy_rows] = 0\n",
    "        dd, vv = sp.linalg.eigs(Adj, k = k)\n",
    "        kmeans = KMeans(n_clusters=k, random_state=0).fit(np.real(vv))\n",
    "        k_mean_results = kmeans.labels_\n",
    "        \n",
    "#        print(\"Stage 1 results\", k_mean_results)\n",
    "        stage1_clustering_results = np.copy(k_mean_results)\n",
    "        \n",
    "        # Stage 2. Majority voting\n",
    "#         k_mean_results = 1-np.array(np.floor(np.arange(0,n)/(n/2)), dtype=int)\n",
    "        \n",
    "        B_est = np.zeros((k, m)) # Caution: The row indices of B_est and B do not match in general\n",
    "        B_ct = np.zeros((k, m, z)) # B_ct(:,:,0) for 0, B_ct(:,:,1) = for 1, and so on, used for finding p_hat\n",
    "        R_ct = np.zeros((k, m, d)) \n",
    "        R_est = np.zeros((k, m)) # estimation of rating matrix from stage 2; u_hat, v_hat\n",
    "        \n",
    "        \n",
    "        for z in range(len(M_obs_locations[0])): # O(pnm)\n",
    "            i = M_obs_locations[0][z] # i\n",
    "            j = M_obs_locations[1][z] # j\n",
    "#             pdb.set_trace()\n",
    "            cluster_idx = k_mean_results[i]\n",
    "            if M_obs[i,j] == -1:\n",
    "                B_ct[cluster_idx, j, 0] += 1\n",
    "            elif M_obs[i,j] == +1:\n",
    "                B_ct[cluster_idx, j, 1] += 1\n",
    "                \n",
    "        for i in range(k):\n",
    "            for j in range(m):\n",
    "                if B_ct[i, j, 1] >= B_ct[i, j, 0]:\n",
    "                    B_est[i,j] = 1\n",
    "                else:\n",
    "                    B_est[i,j] = -1\n",
    "\n",
    "                    \n",
    "        n_ct = 0\n",
    "        diff_ct = 0\n",
    "\n",
    "        for z in range(len(M_obs_locations[0])): # O(pnm)\n",
    "            i = M_obs_locations[0][z] # i\n",
    "            j = M_obs_locations[1][z] # j\n",
    "            cluster_idx = k_mean_results[i]\n",
    "            n_ct += 1\n",
    "            if M_obs[i,j] != B_est[k_mean_results[i],j]:\n",
    "                diff_ct += 1\n",
    "        theta_hat = diff_ct/n_ct\n",
    "#        print(\"theta hat\", theta_hat)\n",
    "        \n",
    "        for i in range(k):\n",
    "            for j in range(m):\n",
    "                if B_est[i,j] == 1:\n",
    "                    R_est[i,j] = 1-theta_hat\n",
    "                else:\n",
    "                    R_est[i,j] = theta_hat\n",
    "                    \n",
    "#        print(\"Stage 2 results\", R_est)\n",
    "\n",
    "        # Stage 3. Local refinement\n",
    "        observed_entries = [None for i in range(n)]\n",
    "        row_sums = Adj_original.sum(axis=1)\n",
    "#        print(\"row_sums\", row_sums)\n",
    "\n",
    "#         for i in range(n):\n",
    "#             observed_entries[i] = np.where(~np.isnan(M_train_arr[i,:]))\n",
    "        \n",
    "        stage3_clustering_results = np.copy(k_mean_results)\n",
    "        edges_per_cluster = np.zeros((n, k))\n",
    "        weighted_sum_of_correct_ratings_per_cluster = np.zeros((n, k))\n",
    "        weighted_sum_of_incorrect_ratings_per_cluster = np.zeros((n, k))\n",
    "        number_of_edges_same_cluster = 0\n",
    "        number_of_edges_diff_cluster = 0\n",
    "        number_of_total_pairs_same_cluster = 0\n",
    "        number_of_total_pairs_diff_cluster = 0\n",
    "\n",
    "        n_per_cluster_stage1_list = []\n",
    "        for i in range(k):\n",
    "            n_per_cluster_stage1_list.append(len(np.where(k_mean_results==i)[0]))\n",
    "            \n",
    "#        print(\"stage3_n\", n)\n",
    "#        print(\"stage1_n_per_cluster\",n_per_cluster_stage1_list)        \n",
    "        \n",
    "        for i in range(k):\n",
    "            number_of_total_pairs_same_cluster += n_per_cluster_stage1_list[i]*(n_per_cluster_stage1_list[i]-1)/2\n",
    "            \n",
    "        for i in range(k):\n",
    "            for j in range(i+1,k):\n",
    "                number_of_total_pairs_diff_cluster += n_per_cluster_stage1_list[i]*n_per_cluster_stage1_list[j]\n",
    "        \n",
    "        for i in range(n):\n",
    "            for j in range(i+1, n):\n",
    "                if k_mean_results[i] == k_mean_results[j]:\n",
    "                    number_of_edges_same_cluster += Adj_original[i,j]\n",
    "                else:\n",
    "                    number_of_edges_diff_cluster += Adj_original[i,j]\n",
    "                \n",
    "#        print(\"number_of_total_pairs_same_cluster\", number_of_total_pairs_same_cluster)\n",
    "#        print(\"number_of_total_pairs_diff_cluster\", number_of_total_pairs_diff_cluster)\n",
    "#        print(\"number_of_edges_same_cluster\", number_of_edges_same_cluster)\n",
    "#        print(\"number_of_edges_diff_cluster\", number_of_edges_diff_cluster)\n",
    "        alpha_hat = number_of_edges_same_cluster/number_of_total_pairs_same_cluster\n",
    "        beta_hat = number_of_edges_diff_cluster/number_of_total_pairs_diff_cluster\n",
    "#        print(\"a hat\", alpha_hat)\n",
    "#        print(\"b hat\", beta_hat)\n",
    "\n",
    "    \n",
    "        if local_refinement_flag:\n",
    "            n_of_refinement_steps = 0\n",
    "\n",
    "            while n_of_refinement_steps <= CVR.MAX_N_OF_REFINEMENT_STEPS:\n",
    "                change_flag = False\n",
    "                n_of_refinement_steps += 1\n",
    "#                print(n_of_refinement_steps)\n",
    "                new_k_mean_results = np.copy(stage3_clustering_results)\n",
    "  \n",
    "                nodes_in_each_cluster = {}\n",
    "                for i in range(k):\n",
    "                    nodes_in_each_cluster[i] = np.where(stage3_clustering_results == i)\n",
    "#                 print nodes_in_each_cluster\n",
    "                    \n",
    "                if n_of_refinement_steps == 1: # initial update\n",
    "                    for i in range(n):\n",
    "                        for j in range(i+1, n): # O(n^2)\n",
    "                            if Adj_original[i,j] == 1:\n",
    "                                edges_per_cluster[i, stage3_clustering_results[j]] += 1\n",
    "                                edges_per_cluster[j, stage3_clustering_results[i]] += 1\n",
    "                    list_of_changes = []\n",
    "                    \n",
    "                    \n",
    "                    for z in range(len(M_obs_locations[0])): # O(pnm)\n",
    "                        i = M_obs_locations[0][z] # i\n",
    "                        j = M_obs_locations[1][z] # j\n",
    "                        for l in range(k):\n",
    "                            if M_obs[i,j] == -1:\n",
    "                                weighted_sum_of_incorrect_ratings_per_cluster[i, l] += np.log(1-R_est[l, j])     \n",
    "                            else:\n",
    "                                weighted_sum_of_correct_ratings_per_cluster[i, l] += np.log(R_est[l, j])    \n",
    "                    \n",
    "                else:\n",
    "                    for i in range(n):\n",
    "                        for each_change in list_of_changes: # O(n)\n",
    "                            j, cluster_old, cluster_new = each_change\n",
    "                            if Adj_original[i,j]:\n",
    "                                edges_per_cluster[i, cluster_old] -= 1\n",
    "                                edges_per_cluster[i, cluster_new] += 1\n",
    "#                     pdb.set_trace()\n",
    "                    list_of_changes = []\n",
    "#                 pdb.set_trace()\n",
    "\n",
    "                n_per_cluster_middle_of_stage3_list = []\n",
    "                for i in range(k):\n",
    "                    n_per_cluster_middle_of_stage3_list.append(len(np.where(new_k_mean_results==i)[0]))\n",
    "\n",
    "#                print(\"n_per_cluster_middle_of_stage3\",n_per_cluster_middle_of_stage3_list) \n",
    "\n",
    "                for i in range(n):\n",
    "#                     print(i)\n",
    "                    likelihood_array = np.zeros(k)\n",
    "                    \n",
    "#                     edges_per_cluster = np.zeros(k)\n",
    "#                     for j in range(n): # O(n^2)\n",
    "#                         if Adj_original[i,j] == 1:\n",
    "#                             edges_per_cluster[stage3_clustering_results[j]] += 1\n",
    "\n",
    "\n",
    "\n",
    "                    for j in range(k): # O(n)\n",
    "                        cluster_idx = j\n",
    "                        deg_internal_1 = edges_per_cluster[i, j]\n",
    "                        deg_internal_0 = n_per_cluster_middle_of_stage3_list[j] -1 - deg_internal_1\n",
    "                        deg_external_1 = np.int(row_sums[i]) - deg_internal_1\n",
    "                        deg_external_0 = n-n_per_cluster_middle_of_stage3_list[j] - deg_external_1\n",
    "                        \n",
    "                        \n",
    "#                         print(\"Node %d, Cluster %d\" % (i,j))\n",
    "#                         print(deg_internal_1, deg_internal_0, deg_external_1, deg_external_0, weighted_sum_of_correct_ratings, weighted_sum_of_incorrect_ratings)\n",
    "\n",
    "                        likelihood_array[j] = \\\n",
    "                                    np.log(alpha_hat) * deg_internal_1 + \\\n",
    "                                    np.log(1-alpha_hat) * deg_internal_0 + \\\n",
    "                                    np.log(beta_hat) * deg_external_1 + \\\n",
    "                                    np.log(1-beta_hat) * deg_external_0 + \\\n",
    "                                    weighted_sum_of_correct_ratings_per_cluster[i, j] + \\\n",
    "                                    weighted_sum_of_incorrect_ratings_per_cluster[i, j]\n",
    "#                     pdb.set_trace()\n",
    "                    opt_clustering_assignment = np.argmax(likelihood_array)\n",
    "                    if opt_clustering_assignment != stage3_clustering_results[i]:\n",
    "                        list_of_changes.append((i, stage3_clustering_results[i], opt_clustering_assignment))\n",
    "                        new_k_mean_results[i] = opt_clustering_assignment\n",
    "                        change_flag = True\n",
    "                        \n",
    "#                         pdb.set_trace()\n",
    "#                         print \"Node %d is removed from %d to %d\" % (i, k_mean_results[i], opt_clustering_assignment)\n",
    "\n",
    "                if not change_flag: # nothing happened\n",
    "                    break\n",
    "\n",
    "                stage3_clustering_results = np.copy(new_k_mean_results)                    \n",
    "                \n",
    "\n",
    "        n_per_cluster_stage3_list = []\n",
    "        for i in range(k):\n",
    "            n_per_cluster_stage3_list.append(len(np.where(stage3_clustering_results==i)[0]))    \n",
    "            \n",
    "#        print(\"stage3_n_per_cluster\", n_per_cluster_stage3_list)\n",
    "\n",
    "\n",
    "        # Stage 4. iteration of theta estimation\n",
    "    \n",
    "        B_est = np.zeros((k, m)) # Caution: The row indices of B_est and B do not match in general\n",
    "        B_ct = np.zeros((k, m, z)) # B_ct(:,:,0) for 0, B_ct(:,:,1) = for 1, and so on, used for finding p_hat\n",
    "        R_ct = np.zeros((k, m, d)) \n",
    "        R_est = np.zeros((k, m)) # estimation of rating matrix from stage 2; u_hat, v_hat\n",
    "        \n",
    "        \n",
    "        for z in range(len(M_obs_locations[0])): # O(pnm)\n",
    "            i = M_obs_locations[0][z] # i\n",
    "            j = M_obs_locations[1][z] # j\n",
    "#             pdb.set_trace()\n",
    "            cluster_idx = stage3_clustering_results[i]\n",
    "            if M_obs[i,j] == -1:\n",
    "                B_ct[cluster_idx, j, 0] += 1\n",
    "            elif M_obs[i,j] == +1:\n",
    "                B_ct[cluster_idx, j, 1] += 1\n",
    "                \n",
    "        for i in range(k):\n",
    "            for j in range(m):\n",
    "                if B_ct[i, j, 1] >= B_ct[i, j, 0]:\n",
    "                    B_est[i,j] = 1\n",
    "                else:\n",
    "                    B_est[i,j] = -1\n",
    "\n",
    "                    \n",
    "        n_ct = 0\n",
    "        diff_ct = 0\n",
    "\n",
    "        for z in range(len(M_obs_locations[0])): # O(pnm)\n",
    "            i = M_obs_locations[0][z] # i\n",
    "            j = M_obs_locations[1][z] # j\n",
    "            cluster_idx = stage3_clustering_results[i]\n",
    "            n_ct += 1\n",
    "            if M_obs[i,j] != B_est[stage3_clustering_results[i],j]:\n",
    "                diff_ct += 1\n",
    "        theta_hat = diff_ct/n_ct\n",
    "#        print(\"theta hat\", theta_hat)\n",
    "        \n",
    "        for i in range(k):\n",
    "            for j in range(m):\n",
    "                if B_est[i,j] == 1:\n",
    "                    R_est[i,j] = 1-theta_hat\n",
    "                else:\n",
    "                    R_est[i,j] = theta_hat\n",
    "                    \n",
    "                    \n",
    "        # Stage 5. Iteration of local refinement \n",
    "        observed_entries = [None for i in range(n)]\n",
    "        row_sums = Adj_original.sum(axis=1)\n",
    "#        print(\"row_sums\", row_sums)\n",
    "\n",
    "#         for i in range(n):\n",
    "#             observed_entries[i] = np.where(~np.isnan(M_train_arr[i,:]))\n",
    "        \n",
    "#        stage3_clustering_results = np.copy(k_mean_results)\n",
    "        edges_per_cluster = np.zeros((n, k))\n",
    "        weighted_sum_of_correct_ratings_per_cluster = np.zeros((n, k))\n",
    "        weighted_sum_of_incorrect_ratings_per_cluster = np.zeros((n, k))\n",
    "        number_of_edges_same_cluster = 0\n",
    "        number_of_edges_diff_cluster = 0\n",
    "        number_of_total_pairs_same_cluster = 0\n",
    "        number_of_total_pairs_diff_cluster = 0\n",
    "\n",
    "        n_per_cluster_stage1_list = []\n",
    "        for i in range(k):\n",
    "            n_per_cluster_stage1_list.append(len(np.where(stage3_clustering_results==i)[0]))\n",
    "            \n",
    "#        print(\"stage3_n\", n)\n",
    "#        print(\"stage1_n_per_cluster\",n_per_cluster_stage1_list)        \n",
    "        \n",
    "        for i in range(k):\n",
    "            number_of_total_pairs_same_cluster += n_per_cluster_stage1_list[i]*(n_per_cluster_stage1_list[i]-1)/2\n",
    "            \n",
    "        for i in range(k):\n",
    "            for j in range(i+1,k):\n",
    "                number_of_total_pairs_diff_cluster += n_per_cluster_stage1_list[i]*n_per_cluster_stage1_list[j]\n",
    "        \n",
    "        for i in range(n):\n",
    "            for j in range(i+1, n):\n",
    "                if stage3_clustering_results[i] == stage3_clustering_results[j]:\n",
    "                    number_of_edges_same_cluster += Adj_original[i,j]\n",
    "                else:\n",
    "                    number_of_edges_diff_cluster += Adj_original[i,j]\n",
    "                \n",
    "#        print(\"number_of_total_pairs_same_cluster\", number_of_total_pairs_same_cluster)\n",
    "#        print(\"number_of_total_pairs_diff_cluster\", number_of_total_pairs_diff_cluster)\n",
    "#        print(\"number_of_edges_same_cluster\", number_of_edges_same_cluster)\n",
    "#        print(\"number_of_edges_diff_cluster\", number_of_edges_diff_cluster)\n",
    "        alpha_hat = number_of_edges_same_cluster/number_of_total_pairs_same_cluster\n",
    "        beta_hat = number_of_edges_diff_cluster/number_of_total_pairs_diff_cluster\n",
    "#        print(\"a hat\", alpha_hat)\n",
    "#        print(\"b hat\", beta_hat)\n",
    "\n",
    "    \n",
    "        if local_refinement_flag:\n",
    "            n_of_refinement_steps = 0\n",
    "\n",
    "            while n_of_refinement_steps <= CVR.MAX_N_OF_REFINEMENT_STEPS:\n",
    "                change_flag = False\n",
    "                n_of_refinement_steps += 1\n",
    "#                print(n_of_refinement_steps)\n",
    "                new_k_mean_results = np.copy(stage3_clustering_results)\n",
    "  \n",
    "                nodes_in_each_cluster = {}\n",
    "                for i in range(k):\n",
    "                    nodes_in_each_cluster[i] = np.where(stage3_clustering_results == i)\n",
    "#                 print nodes_in_each_cluster\n",
    "                    \n",
    "                if n_of_refinement_steps == 1: # initial update\n",
    "                    for i in range(n):\n",
    "                        for j in range(i+1, n): # O(n^2)\n",
    "                            if Adj_original[i,j] == 1:\n",
    "                                edges_per_cluster[i, stage3_clustering_results[j]] += 1\n",
    "                                edges_per_cluster[j, stage3_clustering_results[i]] += 1\n",
    "                    list_of_changes = []\n",
    "                    \n",
    "                    \n",
    "                    for z in range(len(M_obs_locations[0])): # O(pnm)\n",
    "                        i = M_obs_locations[0][z] # i\n",
    "                        j = M_obs_locations[1][z] # j\n",
    "                        for l in range(k):\n",
    "                            if M_obs[i,j] == -1:\n",
    "                                weighted_sum_of_incorrect_ratings_per_cluster[i, l] += np.log(1-R_est[l, j])     \n",
    "                            else:\n",
    "                                weighted_sum_of_correct_ratings_per_cluster[i, l] += np.log(R_est[l, j])    \n",
    "                    \n",
    "                else:\n",
    "                    for i in range(n):\n",
    "                        for each_change in list_of_changes: # O(n)\n",
    "                            j, cluster_old, cluster_new = each_change\n",
    "                            if Adj_original[i,j]:\n",
    "                                edges_per_cluster[i, cluster_old] -= 1\n",
    "                                edges_per_cluster[i, cluster_new] += 1\n",
    "#                     pdb.set_trace()\n",
    "                    list_of_changes = []\n",
    "#                 pdb.set_trace()\n",
    "\n",
    "                n_per_cluster_middle_of_stage3_list = []\n",
    "                for i in range(k):\n",
    "                    n_per_cluster_middle_of_stage3_list.append(len(np.where(new_k_mean_results==i)[0]))\n",
    "\n",
    "#                print(\"n_per_cluster_middle_of_stage3\",n_per_cluster_middle_of_stage3_list) \n",
    "\n",
    "                for i in range(n):\n",
    "#                     print(i)\n",
    "                    likelihood_array = np.zeros(k)\n",
    "                    \n",
    "#                     edges_per_cluster = np.zeros(k)\n",
    "#                     for j in range(n): # O(n^2)\n",
    "#                         if Adj_original[i,j] == 1:\n",
    "#                             edges_per_cluster[stage3_clustering_results[j]] += 1\n",
    "\n",
    "\n",
    "\n",
    "                    for j in range(k): # O(n)\n",
    "                        cluster_idx = j\n",
    "                        deg_internal_1 = edges_per_cluster[i, j]\n",
    "                        deg_internal_0 = n_per_cluster_middle_of_stage3_list[j] -1 - deg_internal_1\n",
    "                        deg_external_1 = np.int(row_sums[i]) - deg_internal_1\n",
    "                        deg_external_0 = n-n_per_cluster_middle_of_stage3_list[j] - deg_external_1\n",
    "                        \n",
    "                        \n",
    "#                         print(\"Node %d, Cluster %d\" % (i,j))\n",
    "#                         print(deg_internal_1, deg_internal_0, deg_external_1, deg_external_0, weighted_sum_of_correct_ratings, weighted_sum_of_incorrect_ratings)\n",
    "\n",
    "                        likelihood_array[j] = \\\n",
    "                                    np.log(alpha_hat) * deg_internal_1 + \\\n",
    "                                    np.log(1-alpha_hat) * deg_internal_0 + \\\n",
    "                                    np.log(beta_hat) * deg_external_1 + \\\n",
    "                                    np.log(1-beta_hat) * deg_external_0 + \\\n",
    "                                    weighted_sum_of_correct_ratings_per_cluster[i, j] + \\\n",
    "                                    weighted_sum_of_incorrect_ratings_per_cluster[i, j]\n",
    "#                     pdb.set_trace()\n",
    "                    opt_clustering_assignment = np.argmax(likelihood_array)\n",
    "                    if opt_clustering_assignment != stage3_clustering_results[i]:\n",
    "                        list_of_changes.append((i, stage3_clustering_results[i], opt_clustering_assignment))\n",
    "                        new_k_mean_results[i] = opt_clustering_assignment\n",
    "                        change_flag = True\n",
    "                        \n",
    "#                         pdb.set_trace()\n",
    "#                         print \"Node %d is removed from %d to %d\" % (i, k_mean_results[i], opt_clustering_assignment)\n",
    "\n",
    "                if not change_flag: # nothing happened\n",
    "                    break\n",
    "\n",
    "                stage3_clustering_results = np.copy(new_k_mean_results)                    \n",
    "                \n",
    "\n",
    "        n_per_cluster_stage3_list = []\n",
    "        for i in range(k):\n",
    "            n_per_cluster_stage3_list.append(len(np.where(stage3_clustering_results==i)[0]))                         \n",
    "                    \n",
    "        return B_est, theta_hat, stage1_clustering_results, stage3_clustering_results, R_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_M_into_train_test(M):\n",
    "    observation_idx = np.where(M!=0)\n",
    "    n_of_total_observations = observation_idx[0].size\n",
    "    n_train = int(0.8 * n_of_total_observations)\n",
    "    n_test = n_of_total_observations - n_train\n",
    "    test_idx = np.random.choice(n_of_total_observations, n_test)\n",
    "    \n",
    "    M_train = M.copy()\n",
    "    M_test = {}\n",
    "    \n",
    "    for each_test_idx in test_idx:\n",
    "        i = observation_idx[0][each_test_idx]\n",
    "        j = observation_idx[1][each_test_idx]\n",
    "        M_train[i,j] = 0\n",
    "        M_test[(i,j)] = M[i,j]\n",
    "    return M_train, M_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Export:\n",
    "    def data(M_data, Adj_list):\n",
    "        f_data = open('librec-plot2/data/ours/rating/rating.txt', 'w')\n",
    "        for each_key in M_data.keys():\n",
    "            f_data.write(\"%d %d %d\\n\" % (each_key[0], each_key[1], M_data[each_key]))\n",
    "        f_data.close()\n",
    "        \n",
    "        f_trust = open('librec-plot2/data/ours/trust/trust.txt', 'w')\n",
    "        for i in range(len(Adj_list[0])):\n",
    "            f_trust.write(\"%d %d %d\\n\" % (Adj_list[1][i], Adj_list[2][i], Adj_list[0][i]))\n",
    "        f_trust.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Grader:\n",
    "    @staticmethod\n",
    "    def measure_accruacy(M_test, stage3_clustering_results, R_est):\n",
    "        mae_rr = 0\n",
    "        mae_rp = 0\n",
    "        mae_pp = 0\n",
    "        acc = 0\n",
    "        lke = 0\n",
    "        n_of_test_entries = len(M_test)\n",
    "#         print(\"n_of_test_entries\", n_of_test_entries)\n",
    "#         print(\"M_test.keys()\", M_test.keys())\n",
    "\n",
    "        for each_key in M_test.keys():\n",
    "            i,j = each_key\n",
    "            mae_rr += abs( 2*(R_est[stage3_clustering_results[i], j] >= 0.5) - 1 - M_test[each_key] )   # FAIR MAE COMPARISON \n",
    "            mae_rp += abs(2*R_est[stage3_clustering_results[i], j] - 1 - M_test[each_key])\n",
    "            mae_pp += abs(R_est[stage3_clustering_results[i], j] - U[cluster_id[i]][j])\n",
    "            acc += 2*(R_est[stage3_clustering_results[i], j] >= 0.5) -1 == M_test[each_key]\n",
    "            if M_test[each_key] == 1:\n",
    "                lke += np.log(R_est[stage3_clustering_results[i], j])\n",
    "            else:\n",
    "                lke += np.log(1-R_est[stage3_clustering_results[i], j])\n",
    "        mae_rr = mae_rr/float(n_of_test_entries)\n",
    "        mae_rp = mae_rp/float(n_of_test_entries)\n",
    "        mae_pp = mae_pp/float(n_of_test_entries)\n",
    "        acc = acc/float(n_of_test_entries)\n",
    "        lke = lke/float(n_of_test_entries)\n",
    "        \n",
    "        return mae_rr, mae_rp, mae_pp, acc, lke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#m = arbitrary version\n",
    "# diff U 0.05 0.95\n",
    "k = 3\n",
    "m = 1000\n",
    "p_gt = np.array([0.05, 0.5, 0.95])\n",
    "#d = p_gt.size\n",
    "d = 3\n",
    "\n",
    "U = np.array([[0.05]*int(m/5)+[0.05]*int(m/5)+[0.5]*int(m/5)+[0.95]*int(m/5)+[0.95]*int(m/5),\n",
    "              [0.05]*int(m/5)+[0.95]*int(m/5)+[0.05]*int(m/5)+[0.05]*int(m/5)+[0.05]*int(m/5),\n",
    "              [0.95]*int(m/5)+[0.95]*int(m/5)+[0.95]*int(m/5)+[0.95]*int(m/5)+[0.95]*int(m/5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_rate = []\n",
    "\n",
    "mae_rr_ours = []\n",
    "mae_rp_ours = []\n",
    "mae_pp_ours = []\n",
    "acc_ours = []\n",
    "lke_ours = []\n",
    "\n",
    "mae_rr_Ahns = []\n",
    "mae_rp_Ahns = []\n",
    "mae_pp_Ahns = []\n",
    "acc_Ahns = []\n",
    "lke_Ahns = []\n",
    "\n",
    "algos = ('ours', 'Ahns', 'itemaverage', 'useraverage', 'userknn', 'itemknn', 'biasedmf', 'soreg', 'trustsvd')\n",
    "\n",
    "maes = [[] for i in range(len(algos))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "encoding = 'utf-8'\n",
    "\n",
    "for t in range(8):\n",
    "    print(t)\n",
    "    now = datetime.now()\n",
    "    current_time = now.strftime(\"%H:%M:%S\")\n",
    "    print(\"Current Time =\", current_time)\n",
    "    p_obs = 0.005+0.005*t\n",
    "    \n",
    "    \n",
    "    mae_rr_ours_tmp = 0\n",
    "    mae_rp_ours_tmp = 0\n",
    "    mae_pp_ours_tmp = 0\n",
    "    acc_ours_tmp = 0\n",
    "    lke_ours_tmp = 0\n",
    "\n",
    "    mae_rr_Ahns_tmp = 0\n",
    "    mae_rp_Ahns_tmp = 0\n",
    "    mae_pp_Ahns_tmp = 0\n",
    "    acc_Ahns_tmp = 0\n",
    "    lke_Ahns_tmp = 0\n",
    "\n",
    "    maes_tmp = [0 for i in range(len(algos))]\n",
    "\n",
    "    for l in range(T):\n",
    "        \n",
    "        dg = Data_Generator(p_obs, p_gt, k, m)\n",
    "        dg.set_U(U)\n",
    "        Adj_matrix, Adj_list, cluster_id, n = dg.generate_graph()\n",
    "        M = dg.generate_rating_data()\n",
    "\n",
    "        M_data = {}\n",
    "        for i in range(M.shape[0]):\n",
    "            for j in range(M.shape[1]):\n",
    "                if M[i,j] != 0:\n",
    "                    M_data[(i,j)] = M[i,j]\n",
    "        M_train, M_test = split_M_into_train_test(M)\n",
    "        Export.data(M_data, Adj_list)\n",
    "\n",
    "\n",
    "        #ours\n",
    "        solver = CVR(M, Adj_matrix, Adj_list, n, m, k, p_gt)\n",
    "        B_est, p_hat, stage1_clustering_results, stage3_clustering_results, R_est = solver.spectral_clustering_and_vote(local_refinement_flag = True)\n",
    "        mae_rr, mae_rp, mae_pp, acc, lke = Grader.measure_accruacy(M_test, stage3_clustering_results, R_est)\n",
    "\n",
    "        mae_rr_ours_tmp += mae_rr\n",
    "        mae_rp_ours_tmp += mae_rp\n",
    "        mae_pp_ours_tmp += mae_pp\n",
    "        acc_ours_tmp += acc\n",
    "        lke_ours_tmp += lke\n",
    "\n",
    "        #Ahns\n",
    "        solver = CVR_Ahns(M, Adj_matrix, Adj_list, n, m, k, p_gt)\n",
    "        B_est, theta_hat, stage1_clustering_results, stage3_clustering_results, R_est = solver.spectral_clustering_and_vote(local_refinement_flag = True)\n",
    "        mae_rr, mae_rp, mae_pp, acc, lke = Grader.measure_accruacy(M_test, stage3_clustering_results, R_est)\n",
    "\n",
    "        mae_rr_Ahns_tmp += mae_rr\n",
    "        mae_rp_Ahns_tmp += mae_rp\n",
    "        mae_pp_Ahns_tmp += mae_pp\n",
    "        acc_Ahns_tmp += acc\n",
    "        lke_Ahns_tmp += lke    \n",
    "\n",
    "        #baseline algorithms\n",
    "        for i in range(len(algos)-2):\n",
    "            process = subprocess.Popen('cd librec-plot2/ && bin/librec rec -exec -conf %s.md' %algos[i+2], shell=True,\n",
    "                                                   stdout=subprocess.PIPE, \n",
    "                                                   stderr=subprocess.PIPE)\n",
    "            # wait for the process to terminate\n",
    "            out, err = process.communicate()\n",
    "            errcode = process.returncode\n",
    "\n",
    "            for line in str(err, encoding).split('\\n'):\n",
    "                if \"MAE\" in line:\n",
    "                    maes_tmp[i+2] += (float(line.split()[-1]))  \n",
    "\n",
    "    obs_rate.append(p_obs)                    \n",
    "    mae_rr_ours.append(mae_rr_ours_tmp/T)\n",
    "    mae_rp_ours.append(mae_rp_ours_tmp/T)\n",
    "    mae_pp_ours.append(mae_pp_ours_tmp/T)\n",
    "    acc_ours.append(acc_ours_tmp/T)\n",
    "    lke_ours.append(lke_ours_tmp/T)\n",
    "\n",
    "    mae_rr_Ahns.append(mae_rr_Ahns_tmp/T)\n",
    "    mae_rp_Ahns.append(mae_rp_Ahns_tmp/T)\n",
    "    mae_pp_Ahns.append(mae_pp_Ahns_tmp/T)\n",
    "    acc_Ahns.append(acc_Ahns_tmp/T)\n",
    "    lke_Ahns.append(lke_Ahns_tmp/T)\n",
    "\n",
    "    for i in range(len(algos)-2):\n",
    "        maes[i+2].append(maes_tmp[i+2]/T)\n",
    "\n",
    "print(obs_rate)\n",
    "print(\"mae_rr_ours\", mae_rr_ours, \"mae_rp_ours\", mae_rp_ours, \"mae_pp_ours\", mae_pp_ours, \"acc_ours\", acc_ours, \"lke_ours\", lke_ours)\n",
    "print(\"mae_rr_Ahns\", mae_rr_Ahns, \"mae_rp_Ahns\", mae_rp_Ahns, \"mae_pp_Ahns\", mae_pp_Ahns, \"acc_Ahns\", acc_Ahns, \"lke_Ahns\", lke_Ahns)\n",
    "print(algos)            \n",
    "print(maes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maes[0] = mae_rp_ours\n",
    "maes[1] = mae_rp_Ahns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as io\n",
    "\n",
    "io.savemat('Facebook_data',{'maes':maes})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as io\n",
    "\n",
    "\n",
    "loaded = io.loadmat('Facebook_data')\n",
    "maes = loaded['maes']\n",
    "algos = ('Ours', 'Ahn\\'s', 'Item Avg', 'User Avg', 'User k-NN', 'Item k-NN', 'Biased MF', 'SoReg', 'Trust SVD')\n",
    "obs_rate = [0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for i, p in enumerate(obs_rate):\n",
    "    for j, algo in enumerate(algos):\n",
    "        result.append([p, algo, maes[j][i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pylab as plt\n",
    "import matplotlib as mpl\n",
    "import pandas as pd\n",
    "\n",
    "plt.rc('font', family='serif', serif='Times')\n",
    "plt.rc('text', usetex=True)\n",
    "plt.rc('xtick', labelsize=11)\n",
    "plt.rc('ytick', labelsize=11)\n",
    "plt.rc('axes', labelsize=11)\n",
    "plt.rc('axes', linewidth=0.5)\n",
    "\n",
    "width = 3.27\n",
    "height = width / 1.618\n",
    "aspect = 1.618\n",
    "\n",
    "\n",
    "mpl.rc(\"figure\", figsize = (width, height))\n",
    "df = pd.DataFrame(result, columns=['p', 'Method', 'MAE'])\n",
    "g = sns.catplot(x='p', y = 'MAE', hue = 'Method', legend = False, palette = ['firebrick','darkgray','b', 'y', 'r', 'm', 'g', 'c', 'k'], data= df, scale = 0.3, height = height, aspect = aspect, kind = 'point', markers = ['o','s','^','<','>','v','x','d','D'] )\n",
    "g.fig.subplots_adjust(left=.18, bottom=.23, right=.99, top=.97)\n",
    "#fig.set_size_inches(width, height)\n",
    "#color = ['firebrick','firebrick','firebrick','firebrick','firebrick','firebrick','firebrick','firebrick','firebrick']\n",
    "plt.xlabel(r'$p$')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend(loc='upper right', fontsize=7, ncol = 3, columnspacing = -0.3, bbox_to_anchor=(1.0, 0.9))\n",
    "plt.axhline(y=0.23621502748930973, color='r', linestyle=':', linewidth=1)\n",
    "#plt.locator_params(axis='x', nbins=8)\n",
    "plt.locator_params(axis='y', nbins=8)\n",
    "#from matplotlib.ticker import AutoMinorLocator, FormatStrFormatter\n",
    "#ax.xaxis.set_minor_locator(AutoMinorLocator())\n",
    "#ax.yaxis.set_minor_locator(AutoMinorLocator())\n",
    "#ax.xaxis.set_minor_formatter(FormatStrFormatter(\"%.3f\"))\n",
    "#plt.ticklabel_format(style='plain', axis='x')\n",
    "plt.axhline(y = 1, color = 'k', linewidth=0.9)\n",
    "plt.axvline(x = 7.49, ymin = 0, ymax = 1, color = 'k', linewidth=0.7)\n",
    "plt.ylim(0.2, 1)\n",
    "plt.savefig('Figure_1_b.eps', format='eps', dpi=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
