{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 2 #number of iterations for each p_obs, put 100 to get the result of the paper (but it takes 4~5 hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generator\n",
    "%matplotlib inline\n",
    "import scipy.io as io\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from numpy import linalg as LA\n",
    "from sklearn.cluster import KMeans\n",
    "import subprocess\n",
    "import time\n",
    "import math\n",
    "import pandas as pd\n",
    "import pdb\n",
    "\n",
    "class Data_Generator:\n",
    "    \"\"\"Generates synthetic data\"\"\"\n",
    "    def __init__(self, p_obs, p_list, k, m):\n",
    "        ## storing input parameters within the class\n",
    "        self.p_obs = p_obs\n",
    "        self.p_list = p_list\n",
    "        self.k = k # number of clusters of users (unequal sizes)\n",
    "        self.m = m\n",
    "        \n",
    "        ## define additional class variables\n",
    "        self.U = np.zeros((k, m)) # u_1, u_2, ..., u_K\n",
    "        self.U_set = False\n",
    "\n",
    "        \n",
    "        self.M_ground_truth = {}\n",
    "        self.M_train = {}\n",
    "        \n",
    "        self.M_train = None\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        z = io.loadmat(\"facebook100/Vassar85.mat\")\n",
    "\n",
    "        idx_list = [[] for i in range(k)]\n",
    "        for i in range(k):\n",
    "            idx_list[i] = np.where((z['local_info'][:, -2] == 2006+i) & (z['local_info'][:, 0] == 1))[0]\n",
    "        idx_2006_2009_student = np.concatenate(idx_list)  # [24,28,30,...15xx, 10,,,14xx, ... 1499,1506,1513]\n",
    "#        print(\"idx_2006_2009_student\",idx_2006_2009_student)\n",
    "#        print(len(idx_2006_2009_student))\n",
    "        B = z['A']\n",
    "#        print(\"B shape\",B.shape)\n",
    "        C = B[idx_2006_2009_student, :] # will be used for finding the largest connected component only\n",
    "        C = C[:, idx_2006_2009_student]\n",
    "#        print(\"C shape\",C.shape)\n",
    "\n",
    "        n_comp, labels = sp.csgraph.connected_components(C)\n",
    "#        print(\"labels\", labels)\n",
    "#        print(\"0\",np.where(labels==0))\n",
    "#        print(idx_2006_2009_student[np.where(labels==0)])\n",
    "        idx_2006_2009_student_large = idx_2006_2009_student[np.where(labels==0)]  # to ignore inactive users (who do not have friends)\n",
    "#        print(\"the number of connected components\", n_comp)\n",
    "#        print(\"the largest connected component\",idx_2006_2009_student_large)\n",
    "#        print(\"length of the largest connected component\",len(idx_2006_2009_student_large))\n",
    "\n",
    "        B_final = z['A'][idx_2006_2009_student_large, :]\n",
    "        B_final = B_final[:, idx_2006_2009_student_large]\n",
    "        #print(\"B_final[0]\", B_final[0])\n",
    "        #print(\"B_final[1136]\", B_final[1136])\n",
    "        #print(\"B_final.shape\",B_final.shape)\n",
    "        D = B_final.toarray()\n",
    "        #print(\"np.where(D[0] == 1)\", np.where(D[0] == 1))\n",
    "        #print(\"np.where(D[1136] == 1\", np.where(D[1136] == 1))\n",
    "        #print(\"D.shape\",D.shape)\n",
    "        # 4 Clusters: Class 06(225), 07(284), 08(351), 09(277)\n",
    "        cluster_id = np.zeros(len(idx_2006_2009_student_large), dtype='int')\n",
    "\n",
    "        for i in range(k):\n",
    "            cluster_id[np.where(z['local_info'][idx_2006_2009_student_large, -2] == 2006+i)] = i\n",
    "\n",
    "        n = len(cluster_id)    \n",
    "        self.n = n\n",
    "#        print(\"n\", n)\n",
    "\n",
    "        Adj_matrix = np.zeros((n,n))   \n",
    "        Adj_list = {}\n",
    "\n",
    "        B_final_locations = np.where(D == 1)\n",
    "        #print(\"B_final_loc\", B_final_locations)\n",
    "\n",
    "        for i in range(n):\n",
    "            Adj_list[i] = []\n",
    "\n",
    "#        print(\"length\", len(B_final_locations[0]))\n",
    "        for y in range(len(B_final_locations[0])): # O(pnm)\n",
    "            i = B_final_locations[0][y] # i\n",
    "            j = B_final_locations[1][y] # j\n",
    "            if  D[i,j] == 1:\n",
    "                Adj_matrix[i,j] = 1\n",
    "                Adj_list[i].append(j)\n",
    "\n",
    "\n",
    "\n",
    "        #print(Adj_matrix)\n",
    "        #print(Adj_list)        \n",
    "        self.Adj_matrix = Adj_matrix\n",
    "        self.Adj_list = Adj_list\n",
    "        \n",
    "        \n",
    "        n_per_cluster_list = []\n",
    "        for i in range(k):\n",
    "            n_per_cluster_list.append(len(np.where(cluster_id==i)[0]))\n",
    "        \n",
    "        self.n_per_cluster_list = n_per_cluster_list\n",
    "#        print(\"real_n_per_cluster_list\",self.n_per_cluster_list)\n",
    "        self.cluster_id = cluster_id\n",
    "        \n",
    "    def set_U(self, U):\n",
    "        self.U = U\n",
    "        self.U_set = True\n",
    "\n",
    "    def generate_rating_data(self):\n",
    "        if self.U_set:\n",
    "            X_full_obs = -1+2*np.array(np.random.random((self.n,self.m)) <= np.repeat(self.U, self.n_per_cluster_list, axis=0), dtype=float)\n",
    "            X_partial_obs = X_full_obs * np.array(np.random.random((self.n,self.m)) <= self.p_obs, dtype=float)\n",
    "            return X_partial_obs\n",
    "        else:\n",
    "            print(\"U is not set yet\")\n",
    "            return None\n",
    "    \n",
    "    def generate_graph(self):\n",
    "        Adj_matrix = self.Adj_matrix\n",
    "        Adj_list = self.Adj_list\n",
    "        cluster_id = self.cluster_id\n",
    "        n = self.n\n",
    "        return Adj_matrix, Adj_list, cluster_id, n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#solver\n",
    "class CVR:\n",
    "    MAX_N_OF_REFINEMENT_STEPS = 10\n",
    "    \n",
    "    def __init__(self, M_obs, Adj_matrix, Adj_list, n, m, k, p_gt):\n",
    "        self.M_obs = M_obs\n",
    "        self.M_obs_locations = np.where(M_obs != 0)\n",
    "        self.Adj_matrix = Adj_matrix\n",
    "        self.Adj_list = Adj_list\n",
    "        self.n = n\n",
    "        self.m = m\n",
    "        self.k = k #number of clusters of users\n",
    "        self.d = p_gt.size\n",
    "        \n",
    "                \n",
    "    def spectral_clustering_and_vote(self, truncation_threshold = 6, local_refinement_flag = False):\n",
    "        M_obs = self.M_obs\n",
    "        M_obs_locations = self.M_obs_locations\n",
    "        Adj = self.Adj_matrix\n",
    "        Adj_original = np.copy(Adj)\n",
    "        \n",
    "        Adj_list = self.Adj_list\n",
    "\n",
    "        n = self.n\n",
    "        m = self.m\n",
    "        k = self.k\n",
    "        d = self.d # number of probabilities p_1,...,p_d\n",
    "        z = 2 # number of possible ratings binary in Alg 1, but will be bigger than 2 in experiment 3\n",
    "        \n",
    "        # Stage 1. Spectral clustering\n",
    "        # Caution: This may be slow for very large n\n",
    "        deg_th = truncation_threshold * np.sum(Adj)/n\n",
    "        heavy_rows = np.where(np.sum(Adj,1) > deg_th)[0]\n",
    "        Adj[heavy_rows,:] = 0\n",
    "        Adj[:,heavy_rows] = 0\n",
    "        dd, vv = sp.linalg.eigs(Adj, k = k)\n",
    "        kmeans = KMeans(n_clusters=k, random_state=0).fit(np.real(vv))\n",
    "        k_mean_results = kmeans.labels_\n",
    "        \n",
    "#        print(\"Stage 1 results\", k_mean_results)\n",
    "        stage1_clustering_results = np.copy(k_mean_results)\n",
    "        \n",
    "        # Stage 2. Majority voting\n",
    "#         k_mean_results = 1-np.array(np.floor(np.arange(0,n)/(n/2)), dtype=int)\n",
    "        \n",
    "        B_est = np.zeros((k, m)) # Caution: The row indices of B_est and B do not match in general\n",
    "        B_ct = np.zeros((k, m, z)) # B_ct(:,:,0) for 0, B_ct(:,:,1) = for 1, and so on, used for finding p_hat\n",
    "        R_ct = np.zeros((k, m, d)) \n",
    "        R_est = np.zeros((k, m)) # estimation of rating matrix from stage 2; u_hat, v_hat\n",
    "        \n",
    "        \n",
    "        for z in range(len(M_obs_locations[0])): # O(pnm)\n",
    "            i = M_obs_locations[0][z] # i\n",
    "            j = M_obs_locations[1][z] # j\n",
    "#             pdb.set_trace()\n",
    "            cluster_idx = k_mean_results[i]\n",
    "            if M_obs[i,j] == -1:\n",
    "                B_ct[cluster_idx, j, 0] += 1\n",
    "            elif M_obs[i,j] == +1:\n",
    "                B_ct[cluster_idx, j, 1] += 1\n",
    "                \n",
    "        for i in range(k):\n",
    "            for j in range(m):\n",
    "                if B_ct[i, j, 1] >= B_ct[i, j, 0]:\n",
    "                    B_est[i,j] = 1\n",
    "                else:\n",
    "                    B_est[i,j] = -1\n",
    "\n",
    "                    \n",
    "        n_ct = 0\n",
    "        diff_ct = 0\n",
    "\n",
    "        for z in range(len(M_obs_locations[0])): # O(pnm)\n",
    "            i = M_obs_locations[0][z] # i\n",
    "            j = M_obs_locations[1][z] # j\n",
    "            cluster_idx = k_mean_results[i]\n",
    "            n_ct += 1\n",
    "            if M_obs[i,j] != B_est[k_mean_results[i],j]:\n",
    "                diff_ct += 1\n",
    "        theta_hat = diff_ct/n_ct\n",
    "#        print(\"theta hat\", theta_hat)\n",
    "        \n",
    "        for i in range(k):\n",
    "            for j in range(m):\n",
    "                if B_est[i,j] == 1:\n",
    "                    R_est[i,j] = 1-theta_hat\n",
    "                else:\n",
    "                    R_est[i,j] = theta_hat\n",
    "                    \n",
    "#        print(\"Stage 2 results\", R_est)\n",
    "\n",
    "        # Stage 3. Local refinement\n",
    "        observed_entries = [None for i in range(n)]\n",
    "        row_sums = Adj_original.sum(axis=1)\n",
    "#        print(\"row_sums\", row_sums)\n",
    "\n",
    "#         for i in range(n):\n",
    "#             observed_entries[i] = np.where(~np.isnan(M_train_arr[i,:]))\n",
    "        \n",
    "        stage3_clustering_results = np.copy(k_mean_results)\n",
    "        edges_per_cluster = np.zeros((n, k))\n",
    "        weighted_sum_of_correct_ratings_per_cluster = np.zeros((n, k))\n",
    "        weighted_sum_of_incorrect_ratings_per_cluster = np.zeros((n, k))\n",
    "        number_of_edges_same_cluster = 0\n",
    "        number_of_edges_diff_cluster = 0\n",
    "        number_of_total_pairs_same_cluster = 0\n",
    "        number_of_total_pairs_diff_cluster = 0\n",
    "\n",
    "        n_per_cluster_stage1_list = []\n",
    "        for i in range(k):\n",
    "            n_per_cluster_stage1_list.append(len(np.where(k_mean_results==i)[0]))\n",
    "            \n",
    "#        print(\"stage3_n\", n)\n",
    "#        print(\"stage1_n_per_cluster\",n_per_cluster_stage1_list)        \n",
    "        \n",
    "        for i in range(k):\n",
    "            number_of_total_pairs_same_cluster += n_per_cluster_stage1_list[i]*(n_per_cluster_stage1_list[i]-1)/2\n",
    "            \n",
    "        for i in range(k):\n",
    "            for j in range(i+1,k):\n",
    "                number_of_total_pairs_diff_cluster += n_per_cluster_stage1_list[i]*n_per_cluster_stage1_list[j]\n",
    "        \n",
    "        for i in range(n):\n",
    "            for j in range(i+1, n):\n",
    "                if k_mean_results[i] == k_mean_results[j]:\n",
    "                    number_of_edges_same_cluster += Adj_original[i,j]\n",
    "                else:\n",
    "                    number_of_edges_diff_cluster += Adj_original[i,j]\n",
    "                \n",
    "#        print(\"number_of_total_pairs_same_cluster\", number_of_total_pairs_same_cluster)\n",
    "#        print(\"number_of_total_pairs_diff_cluster\", number_of_total_pairs_diff_cluster)\n",
    "#        print(\"number_of_edges_same_cluster\", number_of_edges_same_cluster)\n",
    "#        print(\"number_of_edges_diff_cluster\", number_of_edges_diff_cluster)\n",
    "        alpha_hat = number_of_edges_same_cluster/number_of_total_pairs_same_cluster\n",
    "        beta_hat = number_of_edges_diff_cluster/number_of_total_pairs_diff_cluster\n",
    "#        print(\"a hat\", alpha_hat)\n",
    "#        print(\"b hat\", beta_hat)\n",
    "\n",
    "    \n",
    "        if local_refinement_flag:\n",
    "            n_of_refinement_steps = 0\n",
    "\n",
    "            while n_of_refinement_steps <= CVR.MAX_N_OF_REFINEMENT_STEPS:\n",
    "                change_flag = False\n",
    "                n_of_refinement_steps += 1\n",
    "#                print(n_of_refinement_steps)\n",
    "                new_k_mean_results = np.copy(stage3_clustering_results)\n",
    "  \n",
    "                nodes_in_each_cluster = {}\n",
    "                for i in range(k):\n",
    "                    nodes_in_each_cluster[i] = np.where(stage3_clustering_results == i)\n",
    "#                 print nodes_in_each_cluster\n",
    "                    \n",
    "                if n_of_refinement_steps == 1: # initial update\n",
    "                    for i in range(n):\n",
    "                        for j in range(i+1, n): # O(n^2)\n",
    "                            if Adj_original[i,j] == 1:\n",
    "                                edges_per_cluster[i, stage3_clustering_results[j]] += 1\n",
    "                                edges_per_cluster[j, stage3_clustering_results[i]] += 1\n",
    "                    list_of_changes = []\n",
    "                    \n",
    "                    \n",
    "                    for z in range(len(M_obs_locations[0])): # O(pnm)\n",
    "                        i = M_obs_locations[0][z] # i\n",
    "                        j = M_obs_locations[1][z] # j\n",
    "                        for l in range(k):\n",
    "                            if M_obs[i,j] == -1:\n",
    "                                weighted_sum_of_incorrect_ratings_per_cluster[i, l] += np.log(1-R_est[l, j])     \n",
    "                            else:\n",
    "                                weighted_sum_of_correct_ratings_per_cluster[i, l] += np.log(R_est[l, j])    \n",
    "                    \n",
    "                else:\n",
    "                    for i in range(n):\n",
    "                        for each_change in list_of_changes: # O(n)\n",
    "                            j, cluster_old, cluster_new = each_change\n",
    "                            if Adj_original[i,j]:\n",
    "                                edges_per_cluster[i, cluster_old] -= 1\n",
    "                                edges_per_cluster[i, cluster_new] += 1\n",
    "#                     pdb.set_trace()\n",
    "                    list_of_changes = []\n",
    "#                 pdb.set_trace()\n",
    "\n",
    "                n_per_cluster_middle_of_stage3_list = []\n",
    "                for i in range(k):\n",
    "                    n_per_cluster_middle_of_stage3_list.append(len(np.where(new_k_mean_results==i)[0]))\n",
    "\n",
    "#                print(\"n_per_cluster_middle_of_stage3\",n_per_cluster_middle_of_stage3_list) \n",
    "\n",
    "                for i in range(n):\n",
    "#                     print(i)\n",
    "                    likelihood_array = np.zeros(k)\n",
    "                    \n",
    "#                     edges_per_cluster = np.zeros(k)\n",
    "#                     for j in range(n): # O(n^2)\n",
    "#                         if Adj_original[i,j] == 1:\n",
    "#                             edges_per_cluster[stage3_clustering_results[j]] += 1\n",
    "\n",
    "\n",
    "\n",
    "                    for j in range(k): # O(n)\n",
    "                        cluster_idx = j\n",
    "                        deg_internal_1 = edges_per_cluster[i, j]\n",
    "                        deg_internal_0 = n_per_cluster_middle_of_stage3_list[j] -1 - deg_internal_1\n",
    "                        deg_external_1 = np.int(row_sums[i]) - deg_internal_1\n",
    "                        deg_external_0 = n-n_per_cluster_middle_of_stage3_list[j] - deg_external_1\n",
    "                        \n",
    "                        \n",
    "#                         print(\"Node %d, Cluster %d\" % (i,j))\n",
    "#                         print(deg_internal_1, deg_internal_0, deg_external_1, deg_external_0, weighted_sum_of_correct_ratings, weighted_sum_of_incorrect_ratings)\n",
    "\n",
    "                        likelihood_array[j] = \\\n",
    "                                    np.log(alpha_hat) * deg_internal_1 + \\\n",
    "                                    np.log(1-alpha_hat) * deg_internal_0 + \\\n",
    "                                    np.log(beta_hat) * deg_external_1 + \\\n",
    "                                    np.log(1-beta_hat) * deg_external_0 + \\\n",
    "                                    weighted_sum_of_correct_ratings_per_cluster[i, j] + \\\n",
    "                                    weighted_sum_of_incorrect_ratings_per_cluster[i, j]\n",
    "#                     pdb.set_trace()\n",
    "                    opt_clustering_assignment = np.argmax(likelihood_array)\n",
    "                    if opt_clustering_assignment != stage3_clustering_results[i]:\n",
    "                        list_of_changes.append((i, stage3_clustering_results[i], opt_clustering_assignment))\n",
    "                        new_k_mean_results[i] = opt_clustering_assignment\n",
    "                        change_flag = True\n",
    "                        \n",
    "#                         pdb.set_trace()\n",
    "#                         print \"Node %d is removed from %d to %d\" % (i, k_mean_results[i], opt_clustering_assignment)\n",
    "\n",
    "                if not change_flag: # nothing happened\n",
    "                    break\n",
    "\n",
    "                stage3_clustering_results = np.copy(new_k_mean_results)                    \n",
    "                \n",
    "\n",
    "        n_per_cluster_stage3_list = []\n",
    "        for i in range(k):\n",
    "            n_per_cluster_stage3_list.append(len(np.where(stage3_clustering_results==i)[0]))    \n",
    "            \n",
    "#        print(\"stage3_n_per_cluster\", n_per_cluster_stage3_list)\n",
    "        return B_est, theta_hat, stage1_clustering_results, stage3_clustering_results, R_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## parameters\n",
    "\n",
    "k = 4\n",
    "m = 1000\n",
    "p_gt = np.array([0.2, 0.5, 0.7])\n",
    "d = p_gt.size\n",
    "\n",
    "\n",
    "\n",
    "obs_rate = []\n",
    "max_error = []\n",
    "L1_error = []\n",
    "obs_rate_mean = []\n",
    "max_error_mean = []\n",
    "L1_error_mean = []\n",
    "\n",
    "U = np.array([[0.2]*250+[0.5]*250+[0.5]*250+[0.7]*250,[0.2]*250+[0.5]*250+[0.7]*250+[0.5]*250, [0.2]*500+[0.5]*500, [0.5]*500+[0.2]*500])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "for l in range(6):\n",
    "    p_obs = 0.1+0.1*l\n",
    "    mean_max_err = 0\n",
    "    mean_L1_err = 0\n",
    "    print(l)\n",
    "    now = datetime.now()\n",
    "    current_time = now.strftime(\"%H:%M:%S\")\n",
    "    print(\"Current Time =\", current_time)\n",
    "    \n",
    "    for j in range(T):\n",
    "        dg = Data_Generator(p_obs, p_gt, k, m)\n",
    "        dg.set_U(U)\n",
    "        Adj_matrix, Adj_list, cluster_id, n = dg.generate_graph()\n",
    "        solver = CVR(dg.generate_rating_data(), Adj_matrix, Adj_list, n, m, k, p_gt)\n",
    "        B_est, p_hat, stage1_clustering_results, stage3_clustering_results, R_est = solver.spectral_clustering_and_vote(local_refinement_flag = True)\n",
    "        total_err = 0\n",
    "        max_err = 0\n",
    "\n",
    "        for i in range(n):\n",
    "            total_err += np.linalg.norm(R_est[stage3_clustering_results[i]] - U[cluster_id[i]], ord=1)\n",
    "            if np.max(np.abs(R_est[stage3_clustering_results[i]] - U[cluster_id[i]])) > max_err:\n",
    "                max_err = np.max(np.abs(R_est[stage3_clustering_results[i]] - U[cluster_id[i]]))\n",
    "        normalized_L1_norm = total_err/n/m\n",
    "        \n",
    "        obs_rate.append(p_obs)\n",
    "        max_error.append(max_err)\n",
    "        L1_error.append(normalized_L1_norm)\n",
    "        \n",
    "        mean_max_err += max_err\n",
    "        mean_L1_err += normalized_L1_norm\n",
    "        \n",
    "    mean_max_err = mean_max_err/T #it will be 0 after T iterations\n",
    "    mean_L1_err = mean_L1_err/T #it will be 0 after T iterations\n",
    "    \n",
    "    obs_rate_mean.append(p_obs)\n",
    "    max_error_mean.append(mean_max_err)\n",
    "    L1_error_mean.append(mean_L1_err)\n",
    "    \n",
    "print(\"obs_rate\", obs_rate)    \n",
    "print(\"max_error\", max_error) \n",
    "print(\"L1_error\", L1_error)\n",
    "print(\"obs_rate_mean\", obs_rate_mean)\n",
    "print(\"max_error_mean\", max_error_mean)\n",
    "print(\"L1_error_mean\", L1_error_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "dset = pd.DataFrame({'a':obs_rate_mean, 'b':max_error_mean, 'c':L1_error_mean})\n",
    "\n",
    "plt.scatter(dset['a'], dset['b'], color='red', label = \"(max_err)\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel('Observation Rate')\n",
    "plt.ylabel('Error Rate')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(dset['a'], dset['c'], label = \"(L1_err)\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel('Observation Rate')\n",
    "plt.ylabel('Error Rate')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as io\n",
    "\n",
    "io.savemat('data_prev',{'obs':obs_rate, 'max':max_error, 'L1':L1_error, 'obs_mean':obs_rate_mean, 'max_mean':max_error_mean, 'L1_mean':L1_error_mean})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
